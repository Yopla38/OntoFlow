import asyncio
import json
from pathlib import Path
from typing import Dict, Any, List, Tuple
import sys
import nest_asyncio


from agent.Onto_wa_rag.Integration_fortran_RAG import OntoRAG
from agent.Onto_wa_rag.CONSTANT import (
    API_KEY_PATH, CHUNK_SIZE, CHUNK_OVERLAP, ONTOLOGY_PATH_TTL,
    MAX_CONCURRENT, MAX_RESULTS, STORAGE_DIR
)
from agent.Onto_wa_rag.fortran_analysis.providers.consult import FortranEntityExplorer

# --- Imports IPython Magic ---
from IPython.core.magic import Magics, magics_class, line_cell_magic
from IPython.display import display, Markdown

# Appliquer nest_asyncio pour permettre l'utilisation d'asyncio dans un environnement d√©j√† boucl√© (comme Jupyter)
nest_asyncio.apply()


# ==============================================================================
# 1. FONCTIONS D'AFFICHAGE (HELPER FUNCTIONS)
# ==============================================================================

async def show_available_commands():
    """Display all available commands for magic."""
    display(Markdown("""
### ‚ú® ONTORAG - Available Magic Commands ‚ú®

---

#### üîç **Search (Different Modes)**
- **`<question>`**: (Without `/`) **Simple and fast search** with semantic similarity
- **`/search <question>`**: Classic RAG search with generated response
- **`/hierarchical <q>`**: Hierarchical search on multiple levels

---

#### üß† **Unified Agent (In-Depth Analysis)**
- **`/agent <question>`**: **Complete analysis** with unified agent (Fortran + Jupyter)
- **`/agent_reply <response>`**: Reply to a clarification question from the agent
- **`/agent_memory`**: Display current agent memory summary
- **`/agent_clear`**: Clear agent memory
- **`/agent_sources`**: Display all sources consulted in the session

---

#### üìÅ **Document Management**
- **`/add_docs <var_name>`**: Add documents from a Python variable
- **`/list`**: List all indexed documents
- **`/stats`**: Display RAG statistics

---

#### ‚ùì **Help**
- **`/help`**: Display this help message

---

### üéØ **When to use which mode?**

| Mode | Use Case | Speed | Precision |
|------|----------|-------|-----------|
| **Simple search** (`query`) | Quick content search | ‚ö°‚ö°‚ö° | üéØüéØ |
| **Classic search** (`/search`) | Question with generated response | ‚ö°‚ö° | üéØüéØüéØ |
| **Unified agent** (`/agent`) | Complex analysis, multi-file | ‚ö° | üéØüéØüéØüéØ |
| **Hierarchical search** (`/hierarchical`) | Structured search by levels | ‚ö° | üéØüéØüéØ |

"""))


async def display_query_result(result: Dict[str, Any]):
    """Affiche le r√©sultat d'une query() standard."""
    display(Markdown(f"### ü§ñ R√©ponse\n{result.get('answer', 'Pas de r√©ponse')}"))
    sources = result.get('sources', [])
    if sources:
        md = "#### üìö Sources\n"
        for source in sources:
            concepts = source.get('detected_concepts', [])
            concept_str = f"**Concepts**: {', '.join(concepts)}" if concepts else ""
            md += f"- **Fichier**: `{source['filename']}` | **Score**: {source['relevance_score']:.2f} | {concept_str}\n"
        display(Markdown(md))


async def display_hierarchical_result(result: Dict[str, Any]):
    """Affiche les r√©sultats de la recherche hi√©rarchique."""
    display(Markdown(f"### ü§ñ R√©ponse\n{result.get('answer', 'Pas de r√©ponse')}"))
    hierarchical_results = result.get('hierarchical_results', {})
    if hierarchical_results:
        md = "#### üìä R√©sultats par niveau conceptuel\n"
        for level, data in hierarchical_results.items():
            md += f"- **{data.get('display_name', level)}** ({len(data.get('results', []))} r√©sultats):\n"
            for i, res in enumerate(data.get('results', [])[:3]):
                md += f"  - `{res['source_info'].get('filename')}` (sim: {res['similarity']:.2f})\n"
        display(Markdown(md))


async def display_document_list(docs: List[Dict[str, Any]]):
    """Affiche la liste des documents."""
    if not docs:
        display(Markdown("üìÅ Aucun document n'a √©t√© index√©."))
        return
    md = f"### üìÅ {len(docs)} documents index√©s\n"
    for doc in docs:
        md += f"- `{doc.get('filename', 'N/A')}` ({doc.get('total_chunks', 0)} chunks)\n"
    display(Markdown(md))


# ==============================================================================
# 2. CLASSE DE LA MAGIC IPYTHON
# ==============================================================================

@magics_class
class OntoRAGMagic(Magics):
    def __init__(self, shell):
        super(OntoRAGMagic, self).__init__(shell)
        self.rag = None
        self._initialized = False
        print("‚ú® OntoRAG Magic pr√™te. Initialisation au premier usage...")

    async def _initialize_rag(self):
        """Initialisation asynchrone du moteur RAG."""
        print("üöÄ Initialisation du moteur OntoRAG (une seule fois)...")
        self.rag = OntoRAG(storage_dir=STORAGE_DIR, ontology_path=ONTOLOGY_PATH_TTL)
        await self.rag.initialize()
        self._initialized = True
        print("‚úÖ Moteur OntoRAG initialis√© et pr√™t.")

    async def _handle_agent_run(self, user_input: str):
        """G√®re un tour de conversation avec l'agent unifi√©."""
        print("üß† L'agent r√©fl√©chit...")
        retriever = self.rag.unified_agent.semantic_retriever

        # R√©indexation √† la demande si n√©cessaire
        if len(retriever.chunks) == 0:
            print("  üîÑ Index vide, construction automatique...")
            notebook_count = retriever.build_index_from_existing_chunks(self.rag)

        # ‚úÖ UTILISER L'AGENT UNIFI√â avec la version structur√©e
        agent_response = await self.rag.unified_agent.run(user_input, use_memory=True)

        if agent_response.status == "clarification_needed":
            question_from_agent = agent_response.clarification_question
            display(Markdown(f"""### ‚ùì L'agent a besoin d'une clarification
    > {question_from_agent}

    **Pour r√©pondre, utilisez la commande :** `%rag /agent_reply <votre_r√©ponse>`"""))

        elif agent_response.status == "success":
            # Affichage enrichi avec les m√©tadonn√©es
            display(Markdown(f"### ‚úÖ R√©ponse finale de l'agent\n{agent_response.answer}"))

            # Afficher les sources automatiquement
            if agent_response.sources_consulted:
                sources_md = "\n## üìö Sources consult√©es :\n"
                for source in agent_response.sources_consulted:
                    sources_md += f"\n{source.get_citation()}"
                display(Markdown(sources_md))

            # Afficher les m√©tadonn√©es utiles
            metadata_md = f"""
    ### üìä M√©tadonn√©es de la r√©ponse
    - ‚è±Ô∏è **Temps d'ex√©cution**: {agent_response.execution_time_total_ms:.0f}ms
    - üî¢ **√âtapes utilis√©es**: {agent_response.steps_taken}/{agent_response.max_steps}
    - üìö **Sources consult√©es**: {len(agent_response.sources_consulted)}
    - üéØ **Niveau de confiance**: {agent_response.confidence_level:.2f}
    """

            # Ajouter les questions de suivi sugg√©r√©es
            if agent_response.suggested_followup_queries:
                metadata_md += f"\n### üí° Questions de suivi sugg√©r√©es :\n"
                for i, suggestion in enumerate(agent_response.suggested_followup_queries[:3], 1):
                    metadata_md += f"{i}. {suggestion}\n"

            display(Markdown(metadata_md))
            print("\n‚úÖ Conversation termin√©e. Pour une nouvelle question, utilisez √† nouveau `/agent`.")

        elif agent_response.status == "timeout":
            display(Markdown(f"""### ‚è∞ Timeout de l'agent
    L'agent a atteint la limite de temps mais a trouv√© des informations partielles :

    {agent_response.answer}"""))

            if agent_response.sources_consulted:
                sources_md = "\n## üìö Sources consult√©es malgr√© le timeout :\n"
                for source in agent_response.sources_consulted:
                    sources_md += f"\n{source.get_citation()}"
                display(Markdown(sources_md))

        elif agent_response.status == "error":
            display(Markdown(f"""### ‚ùå Erreur de l'agent
    {agent_response.error_details}

    Essayez de reformuler votre question ou utilisez `/help` pour voir les commandes disponibles."""))

        else:
            display(Markdown(f"### ‚ö†Ô∏è Statut inattendu : {agent_response.status}"))

    async def _handle_simple_search(self, query: str, max_results: int = 5):
        """Effectue une recherche simple + g√©n√©ration de r√©ponse avec le LLM."""
        print(f"üîç Recherche simple RAG : '{query}'")

        # V√©rifier si l'agent unifi√© est disponible
        if not hasattr(self.rag, 'unified_agent') or not self.rag.unified_agent:
            display(Markdown("‚ùå **Agent unifi√© non disponible**\n\nUtilisez `/search` pour la recherche classique."))
            return

        retriever = self.rag.unified_agent.semantic_retriever

        # R√©indexation √† la demande si n√©cessaire
        if len(retriever.chunks) == 0:
            print("  üîÑ Index vide, construction automatique...")
            notebook_count = retriever.build_index_from_existing_chunks(self.rag)

            if notebook_count == 0:
                display(Markdown(f"""‚ùå **Aucun notebook disponible**

    Les documents index√©s ne contiennent pas de notebooks Jupyter (.ipynb).

    **Alternatives :**
    - `/search {query}` pour la recherche classique
    - `/list` pour voir les documents disponibles"""))
                return

        # 1. Effectuer la recherche s√©mantique
        results = retriever.query(query, k=max_results)

        if not results:
            display(Markdown(f"""### üîç Recherche : "{query}"

    ‚ùå **Aucun r√©sultat trouv√©** (seuil de similarit√© : 0.25)

    **Suggestions :**
    - Essayez des termes plus g√©n√©raux
    - `/agent {query}` pour une analyse approfondie  
    - `/search {query}` pour la recherche classique"""))
            return

        # 2. G√©n√©rer la r√©ponse avec le LLM
        print(f"  ü§ñ G√©n√©ration de la r√©ponse avec {len(results)} chunks de contexte...")

        try:
            answer, sources_info = await self._generate_rag_response(query, results)

            # 3. Afficher la r√©ponse compl√®te
            await self._display_rag_response(query, answer, results, sources_info)

        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration r√©ponse: {e}")
            # Fallback : afficher les chunks bruts
            display(Markdown("‚ö†Ô∏è **Erreur de g√©n√©ration LLM**, affichage des chunks bruts :"))
            await self._display_simple_search_results(query, results)

    async def _generate_rag_response(self, query: str, results: List[Dict[str, Any]]) -> Tuple[
        str, List[Dict[str, Any]]]:
        """G√©n√®re une r√©ponse avec le LLM √† partir des chunks trouv√©s."""

        # 1. Construire le contexte depuis les chunks
        context_parts = []
        sources_info = []

        for i, result in enumerate(results, 1):
            source_filename = result.get("source_filename", "Unknown")
            content = result.get("content", "")
            similarity_score = result.get("similarity_score", 0.0)

            # Ajouter au contexte
            context_parts.append(f"[Source {i}] De {source_filename} (score: {similarity_score:.3f}):\n{content}")

            # Info pour les sources finales
            sources_info.append({
                "index": i,
                "filename": source_filename,
                "score": similarity_score,
                "source_file": result.get("source_file", ""),
                "tokens": result.get("tokens", "?")
            })

        context = "\n\n".join(context_parts)

        # 2. Construire le prompt pour le LLM
        system_prompt = """You are an expert assistant who answers questions by ALWAYS citing your sources.

You have access to the following sources from Jupyter notebooks:
- OBLIGATORILY cite your sources using [Source N] in your response
- Focus on the most relevant information
- Structure your response in a clear and practical manner
- Do not invent any information that is not found in the sources

Example citation: "According to [Source 1], to create a molecule..."
    """

        user_prompt = f"""Question: {query}

    Avalaible context:
    {context}

    Answer the question using exclusively the information from the context and citing your sources [Source N]."""

        # 3. Appeler le LLM
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        llm_response = await self.rag.rag_engine.llm_provider.generate_response(messages, temperature=0.3)

        return llm_response, sources_info

    async def _display_rag_response(self, query: str, answer: str, results: List[Dict], sources_info: List[Dict]):
        """Affiche la r√©ponse RAG compl√®te avec m√©tadonn√©es."""

        # En-t√™te avec statistiques
        total_indexed = len(self.rag.unified_agent.semantic_retriever.chunks)
        indexed_files_count = len(self.rag.unified_agent.semantic_retriever.indexed_files)
        avg_score = sum(r.get("similarity_score", 0) for r in results) / len(results)

        header = f"""### ü§ñ R√©ponse RAG : "{query}"

    üìä **Contexte :** {len(results)} chunks s√©lectionn√©s sur {total_indexed} disponibles ({indexed_files_count} notebooks) | **Score moyen :** {avg_score:.3f}

    ---

    """

        # Corps de la r√©ponse
        response_body = f"""### üí° R√©ponse

    {answer}

    ---

    """

        # Sources d√©taill√©es
        sources_section = "### üìö Sources utilis√©es\n\n"
        for source in sources_info:
            score_bar = "üü¢" * int(source["score"] * 10) + "‚ö™" * (10 - int(source["score"] * 10))
            sources_section += f"""**[Source {source['index']}]** `{source['filename']}` | Score: {source['score']:.3f} {score_bar} | Tokens: {source['tokens']}\n\n"""

        # Actions sugg√©r√©es
        footer = f"""
    ---

    ### üí° Actions sugg√©r√©es

    - **Analyse approfondie :** `%rag /agent {query}`
    - **Plus de contexte :** `%rag /simple_search_more {query}`  
    - **Voir les chunks bruts :** `%rag /simple_search_raw {query}`
    """

        # Afficher tout
        display(Markdown(header + response_body + sources_section))

    async def _display_simple_search_results(self, query: str, results: List[Dict[str, Any]]):
        """Affiche les r√©sultats de la recherche simple de mani√®re attractive."""

        # En-t√™te avec statistiques
        total_indexed = len(self.rag.unified_agent.semantic_retriever.chunks)
        indexed_files = len(self.rag.unified_agent.semantic_retriever.indexed_files)

        header = f"""### üîç R√©sultats de recherche : "{query}"

    üìä **{len(results)} r√©sultat(s) trouv√©(s)** sur {total_indexed} chunks index√©s ({indexed_files} notebooks)

    ---
    """
        display(Markdown(header))

        # Afficher chaque r√©sultat
        for i, result in enumerate(results, 1):
            score = result["similarity_score"]
            source_file = result["source_filename"]
            tokens = result.get("tokens", "?")
            content = result["content"]

            # Tronquer le contenu si trop long
            if len(content) > 800:
                content_preview = content[:800] + "\n\n*[...contenu tronqu√©...]*"
            else:
                content_preview = content

            # Barre de score visuelle
            score_bar = "üü¢" * int(score * 10) + "‚ö™" * (10 - int(score * 10))

            result_md = f"""
    #### üìÑ R√©sultat {i}/{len(results)}

    **üìÅ Source :** `{source_file}` | **üéØ Score :** {score:.3f} {score_bar} | **üìù Tokens :** {tokens}

    ```
    {content_preview}
    ```

    ---
    """
            display(Markdown(result_md))

    @line_cell_magic
    def rag(self, line, cell=None):
        """Magic command principale pour interagir avec OntoRAG."""

        async def main():
            if not self._initialized:
                await self._initialize_rag()

            query = cell.strip() if cell else line.strip()
            if not query:
                await show_available_commands()
                return

            parts = query.split(' ', 1)
            command = parts[0]
            args = parts[1].strip() if len(parts) > 1 else ""

            try:
                if command.startswith('/'):
                    # --- COMMANDES DE L'AGENT UNIFI√â ---
                    if command == '/agent':
                        print("üí´ Commande : /agent", args)
                        await self._handle_agent_run(args)

                    elif command == '/agent_reply':
                        print("üí¨ R√©ponse utilisateur :", args)
                        await self._handle_agent_run(args)

                    elif command == '/agent_memory':
                        """Affiche le r√©sum√© de la m√©moire de l'agent."""
                        memory_summary = self.rag.unified_agent.get_memory_summary()
                        display(Markdown(f"### üß† M√©moire de l'agent\n{memory_summary}"))

                    elif command == '/agent_clear':
                        """Efface la m√©moire de l'agent."""
                        self.rag.unified_agent.clear_memory()
                        display(Markdown("### üßπ M√©moire de l'agent effac√©e"))

                    elif command == '/agent_sources':
                        """Affiche toutes les sources utilis√©es dans la session."""
                        sources = self.rag.unified_agent.get_sources_used()
                        if sources:
                            sources_md = f"### üìö Sources de la session ({len(sources)} r√©f√©rences)\n"
                            for source in sources:
                                sources_md += f"\n{source.get_citation()}"
                            display(Markdown(sources_md))
                        else:
                            display(Markdown("### üìö Aucune source consult√©e dans cette session"))

                    elif command == '/add_docs':
                        var_name = args.strip()
                        documents_to_add = self.shell.user_ns.get(var_name)
                        if documents_to_add is None:
                            print(f"‚ùå Variable '{var_name}' non trouv√©e.")
                            return
                        print(f"üìö Ajout de {len(documents_to_add)} documents...")
                        results = await self.rag.add_documents_batch(documents_to_add, max_concurrent=MAX_CONCURRENT)
                        print(f"‚úÖ Ajout termin√©: {sum(results.values())}/{len(results)} succ√®s.")

                    elif command == '/list':
                        docs = self.rag.list_documents()
                        await display_document_list(docs)

                    elif command == '/stats':
                        stats = self.rag.get_statistics()
                        display(Markdown(f"### üìä Statistiques OntoRAG\n```json\n{json.dumps(stats, indent=2)}\n```"))

                    elif command == '/search':
                        result = await self.rag.query(args, max_results=MAX_RESULTS)
                        await display_query_result(result)

                    elif command == '/hierarchical':
                        result = await self.rag.hierarchical_query(args)
                        await display_hierarchical_result(result)

                    elif command == '/help':
                        await show_available_commands()

                    else:
                        print(f"‚ùå Commande inconnue: '{command}'.")
                        await show_available_commands()

                else:  # Requ√™te en langage naturel directe
                    print("ü§ñ Requ√™te directe via SimpleRetriever...")
                    await self._handle_simple_search(query, max_results=5)

            except Exception as e:
                print(f"‚ùå Une erreur est survenue: {e}")
                import traceback
                traceback.print_exc()

        asyncio.run(main())


# ==============================================================================
# 3. FONCTION DE CHARGEMENT IPYTHON
# ==============================================================================

def load_ipython_extension(ipython):
    """Enregistre la classe de magics lors du chargement de l'extension."""
    ipython.register_magics(OntoRAGMagic)