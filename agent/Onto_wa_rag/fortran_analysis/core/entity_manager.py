"""
    ------------------------------------------
    Copyright: CEA Grenoble
    Auteur: Yoann CURE
    Entit√©: IRIG
    Ann√©e: 2025
    Description: Agent IA d'Int√©gration Continue
    ------------------------------------------
    """

# core/entity_manager.py
"""
Gestionnaire unifi√© des entit√©s Fortran.
Remplace et unifie la logique de regroupement du visualiseur et de l'EntityIndex.
"""

import asyncio
import os
import pickle
import re
import logging
from typing import Dict, List, Set, Optional, Any, Tuple, Literal
from dataclasses import dataclass, field
from collections import defaultdict

from ..utils.chunk_access import ChunkAccessManager
from ..utils.caching import global_cache

logger = logging.getLogger(__name__)


@dataclass
class UnifiedEntity:
    """Entit√© Fortran unique - remplace FortranEntity et UnifiedEntity"""
    # === DONN√âES DE BASE (ex-FortranEntity) ===
    entity_name: str
    entity_type: str
    start_line: int
    end_line: int

    # === M√âTADONN√âES ESSENTIELLES ===
    entity_id: str = ""  # G√©n√©r√© automatiquement si vide
    filepath: str = ""
    filename: str = ""
    parent_entity: Optional[str] = None
    signature: str = ""
    source_method: str = "hybrid"
    confidence: float = 1.0
    access_level: Optional[str] = None  # Public or private
    arguments: Optional = None
    return_type: Optional = None

    # === RELATIONS (Sets pour compatibilit√©) ===
    dependencies: List[Dict[str, Any]] = field(default_factory=list)# Set[str] = field(default_factory=set)
    called_functions: List[Dict[str, Any]] = field(default_factory=list)
    concepts: Set[str] = field(default_factory=set)

    qualified_name: Optional[str] = ""  # <- AJOUTER
    chunk_ids: Set[str] = field(default_factory=set)  # <- AJOUTER
    all_chunk_ids: List[str] = field(default_factory=list)  # <- AJOUTER

    # === REGROUPEMENT (optionnel, d√©fauts simples) ===
    chunks: List[Dict[str, Any]] = field(default_factory=list)
    detected_concepts: List[Dict[str, Any]] = field(default_factory=list)
    is_complete: bool = True
    is_grouped: bool = False
    source_code: str = ""
    entity_role: Literal['summary', 'documentation', 'code', 'header', 'default'] = 'default'
    notebook_summary: str = ""

    # === PROPRI√âT√âS CALCUL√âES ===
    @property
    def base_name(self) -> str:
        """Nom sans suffixes _part_X"""
        return re.sub(r'_part_\d+$', '', self.entity_name)

    @property
    def name(self) -> str:
        """Alias pour compatibilit√© avec FortranEntity"""
        return self.entity_name

    @property
    def parent(self) -> Optional[str]:
        """Alias pour compatibilit√© avec FortranEntity"""
        return self.parent_entity

    def __post_init__(self):
        """G√©n√®re entity_id si manquant"""
        if not self.entity_id:
            self.entity_id = f"{self.filepath}#{self.entity_type}#{self.entity_name}#{self.start_line}"

    # === FACTORY METHODS ===
    @classmethod
    def from_parser_result(cls, name: str, entity_type: str, start_line: int, end_line: int,
                           filepath: str = "", **kwargs) -> 'UnifiedEntity':
        """Cr√©e une entit√© depuis le parser (remplace FortranEntity)"""
        return cls(
            entity_name=name,
            entity_type=entity_type,
            start_line=start_line,
            end_line=end_line,
            filepath=filepath,
            filename=filepath.split('/')[-1] if filepath else "",
            **kwargs
        )

    def to_dict(self) -> Dict[str, Any]:
        """Format dict compatible avec l'existant"""
        return {
            'name': self.entity_name,
            'type': self.entity_type,
            'start_line': self.start_line,
            'end_line': self.end_line,
            'parent': self.parent_entity,
            'dependencies': list(self.dependencies),
            'called_functions': self.called_functions,
            'signature': self.signature,
            'source_method': self.source_method,
            'confidence': self.confidence,
            'filepath': self.filepath,
            'entity_id': self.entity_id,
            'is_grouped': self.is_grouped,
            'chunk_count': len(self.chunks),
            'access_level': self.access_level,
            'entity_role': self.entity_role,
            'notebook_summary': self.notebook_summary,
            'source_code_preview': self.source_code
        }


class EntityManager:
    """
    Gestionnaire unifi√© des entit√©s Fortran.
    Combine et am√©liore la logique de l'EntityIndex et du visualiseur.
    """

    def __init__(self, document_store):
        self.document_store = document_store
        self.chunk_access = ChunkAccessManager(document_store)

        # Index principal : nom -> entity_id
        self.name_to_entity: Dict[str, str] = {}

        # Entit√©s unifi√©es : entity_id -> UnifiedEntity
        self.entities: Dict[str, UnifiedEntity] = {}

        # Relations hi√©rarchiques
        self.parent_to_children: Dict[str, List[str]] = defaultdict(list)
        self.child_to_parent: Dict[str, str] = {}

        # Index par fichier : filepath -> [entity_ids]
        self.file_to_entities: Dict[str, List[str]] = defaultdict(list)

        # Index par type : entity_type -> [entity_ids]
        self.type_to_entities: Dict[str, List[str]] = defaultdict(list)

        # Cache pour les recherches fr√©quentes
        self._search_cache: Dict[str, List[str]] = {}

        self.persistence_path = None
        self._initialized = False

    async def initialize(self):
        if self._initialized:
            return

        # Stocke le chemin de persistance pour une utilisation future (save/load)
        self.persistence_path = os.path.join(self.document_store.storage_dir, 'entity_manager.pkl')

        # Essayer de charger l'√©tat sauvegard√©
        if os.path.exists(self.persistence_path):
            try:
                self.load_state()
                self._initialized = True
                logger.info(f"‚úÖ EntityManager restaur√© depuis {self.persistence_path}: {len(self.entities)} entit√©s")
                return
            except Exception as e:
                logger.error(f"√âchec de la restauration de l'√©tat, reconstruction compl√®te... Erreur: {e}")

        # Si le chargement √©choue, reconstruire comme avant (fallback)
        logger.info("üîß Aucun √©tat sauvegard√© trouv√©. Initialisation compl√®te de l'EntityManager...")
        await self._build_unified_index()
        await self._build_relationships()
        await self._detect_and_group_split_entities()

        # Sauvegarder le nouvel √©tat pour la prochaine fois
        self.save_state()

    def save_state(self):
        """
        Sauvegarde l'√©tat complet de l'EntityManager dans un fichier.
        Utilise pickle pour une s√©rialisation rapide et efficace.
        """
        logger.info(f"üíæ Sauvegarde de l'√©tat de l'EntityManager dans {self.persistence_path}...")
        try:
            # Cr√©er le r√©pertoire parent s'il n'existe pas
            parent_dir = os.path.dirname(self.persistence_path)
            if parent_dir:
                os.makedirs(parent_dir, exist_ok=True)

            # Regrouper tous les dictionnaires d'index dans un seul objet d'√©tat
            state = {
                'entities': self.entities,
                'name_to_entity': self.name_to_entity,
                'parent_to_children': self.parent_to_children,
                'child_to_parent': self.child_to_parent,
                'file_to_entities': self.file_to_entities,
                'type_to_entities': self.type_to_entities,
            }

            # √âcrire l'objet d'√©tat dans le fichier en mode binaire
            with open(self.persistence_path, 'wb') as f:
                pickle.dump(state, f)

            logger.info(f"‚úÖ √âtat de l'EntityManager sauvegard√© avec succ√®s ({len(self.entities)} entit√©s).")

        except (IOError, pickle.PicklingError) as e:
            logger.error(f"‚ùå √âchec de la sauvegarde de l'√©tat de l'EntityManager : {e}", exc_info=True)

    def load_state(self):
        """
        Charge l'√©tat complet de l'EntityManager depuis un fichier.
        L√®ve une exception si le fichier n'existe pas ou si le chargement √©choue.
        """
        if not os.path.exists(self.persistence_path):
            raise FileNotFoundError(f"Le fichier d'√©tat {self.persistence_path} n'a pas √©t√© trouv√©.")

        logger.info(f"üîÑ Chargement de l'√©tat de l'EntityManager depuis {self.persistence_path}...")
        try:
            # Lire le fichier en mode binaire
            with open(self.persistence_path, 'rb') as f:
                state = pickle.load(f)

            # Restaurer tous les index depuis l'objet d'√©tat charg√©
            self.entities = state.get('entities', {})
            self.name_to_entity = state.get('name_to_entity', {})
            self.parent_to_children = state.get('parent_to_children', defaultdict(list))
            self.child_to_parent = state.get('child_to_parent', {})
            self.file_to_entities = state.get('file_to_entities', defaultdict(list))
            self.type_to_entities = state.get('type_to_entities', defaultdict(list))

            for entity in self.entities.values():
                if not hasattr(entity, 'entity_role'):
                    entity.entity_role = 'default'
                if not hasattr(entity, 'notebook_summary'):
                    entity.notebook_summary = ''
                if not hasattr(entity, 'source_code'):
                    entity.source_code = ''

            logger.info(f"‚úÖ √âtat de l'EntityManager restaur√© avec succ√®s ({len(self.entities)} entit√©s).")

        except (IOError, pickle.UnpicklingError, KeyError) as e:
            logger.error(
                f"‚ùå √âchec du chargement de l'√©tat de l'EntityManager. Le fichier est peut-√™tre corrompu. Erreur : {e}",
                exc_info=True)
            # Propage l'erreur pour que la m√©thode appelante sache que le chargement a √©chou√©
            raise

    async def add_document_entities(self, document_id: str):
        """Ajoute les entit√©s d'un document √† l'index"""
        try:
            await self._index_document_entities(document_id)
            logger.info(f"Entit√©s du document {document_id} index√©es")
        except Exception as e:
            logger.error(f"Erreur indexation document {document_id}: {e}")

    async def remove_document_entities(self, document_id: str):
        """Supprime les entit√©s d'un document de l'index"""
        try:
            # Trouver les entit√©s de ce document
            entities_to_remove = []
            for entity_id, entity in self.entities.items():
                # V√©rifier si l'entit√© appartient √† ce document
                if any(chunk_info.get('chunk_id', '').startswith(f"{document_id}-")
                       for chunk_info in entity.chunks):
                    entities_to_remove.append(entity_id)

            # Supprimer les entit√©s
            for entity_id in entities_to_remove:
                if entity_id in self.entities:
                    entity = self.entities[entity_id]

                    # Nettoyer les index
                    self.name_to_entity.pop(entity.entity_name.lower(), None)

                    if entity.filepath in self.file_to_entities:
                        if entity_id in self.file_to_entities[entity.filepath]:
                            self.file_to_entities[entity.filepath].remove(entity_id)

                    if entity.entity_type in self.type_to_entities:
                        if entity_id in self.type_to_entities[entity.entity_type]:
                            self.type_to_entities[entity.entity_type].remove(entity_id)

                    # Supprimer l'entit√©
                    del self.entities[entity_id]

            logger.info(f"Supprim√© {len(entities_to_remove)} entit√©s du document {document_id}")

        except Exception as e:
            logger.error(f"Erreur suppression entit√©s document {document_id}: {e}")

    async def list_entities(self, limit: int = 10) -> List[UnifiedEntity]:
        """Liste les entit√©s (nouvelle m√©thode)"""
        all_entities = list(self.entities.values())
        return all_entities[:limit]

    def get_all_entities(self) -> List[UnifiedEntity]:
        """
        Retourne une liste de toutes les entit√©s UnifiedEntity g√©r√©es.
        C'est la m√©thode √† utiliser pour obtenir une collection compl√®te
        √† des fins de filtrage ou d'it√©ration.
        """
        # self.entities est un dictionnaire {entity_id: UnifiedEntity}
        # .values() nous donne une collection de tous les objets UnifiedEntity
        return list(self.entities.values())

    async def _build_unified_index(self):
        """Construit l'index unifi√© depuis tous les documents"""
        all_docs = await self.document_store.get_all_documents()

        for doc_id in all_docs:
            await self._index_document_entities(doc_id)

    async def _index_document_entities(self, document_id: str):
        """Indexe les entit√©s d'un document"""
        chunks = await self.chunk_access.get_chunks_by_document(document_id)

        for chunk in chunks:
            entity_info = await self.chunk_access.get_entity_info_from_chunk(chunk['id'])
            if not entity_info or not entity_info.get('name'):
                continue

            await self._add_entity_from_chunk(chunk, entity_info)

    async def _add_entity_from_chunk(self, chunk: Dict[str, Any], entity_info: Dict[str, Any]):
        """Ajoute une entit√© depuis un chunk"""
        chunk_id = chunk['id']
        entity_name = entity_info['name']
        base_name = re.sub(r'_part_\d+$', '', entity_name)

        # D√©terminer l'ID unique de l'entit√©
        entity_id = self._generate_entity_id(entity_info, base_name)

        # Cr√©er ou r√©cup√©rer l'entit√© unifi√©e
        if entity_id not in self.entities:
            self.entities[entity_id] = UnifiedEntity(
                entity_id=entity_id,
                entity_name=base_name,
                entity_type=entity_info.get('type', 'unknown'),
                filepath=entity_info.get('filepath', ''),
                filename=entity_info.get('filename', ''),
                start_line=entity_info.get('start_pos'),
                end_line=entity_info.get('end_pos'),
                parent_entity=entity_info.get('parent', ''),
                qualified_name=entity_info.get('qualified_name', ''),
                signature=entity_info.get('signature', ''),
                source_method=entity_info.get('source_method', 'chunk_metadata'),
                access_level=entity_info.get('access_level'),
                called_functions=entity_info.get('called_functions'),
                entity_role=entity_info.get('entity_role', 'default'),
                notebook_summary=entity_info.get('notebook_summary', ''),
                source_code=entity_info.get('source_code', '')

            )

        entity = self.entities[entity_id]

        # Ajouter le chunk √† l'entit√©
        chunk_info = {
            'chunk_id': chunk_id,
            'entity_info': entity_info,
            'part_index': self._extract_part_index(entity_name),
            'start_line': entity_info.get('start_line'),
            'end_line': entity_info.get('end_line')
        }

        entity.chunks.append(chunk_info)
        entity.chunk_ids.add(chunk_id)
        entity.all_chunk_ids.append(chunk_id)

        # Agr√©ger les m√©tadonn√©es
        entity.dependencies.update(entity_info.get('dependencies', []))
        entity.concepts.update(self._extract_concept_labels(entity_info.get('concepts', [])))
        entity.detected_concepts.extend(entity_info.get('detected_concepts', []))

        entity.called_functions.update(entity_info.get('called_functions', []))

        # Mettre √† jour les bounds
        if entity_info.get('start_pos'):
            if not entity.start_line or entity_info['start_pos'] < entity.start_line:
                entity.start_line = entity_info['start_pos']
        if entity_info.get('end_pos'):
            if not entity.end_line or entity_info['end_pos'] > entity.end_line:
                entity.end_line = entity_info['end_pos']

        # Indexer par nom
        self.name_to_entity[base_name.lower()] = entity_id
        self.name_to_entity[entity_name.lower()] = entity_id

        # Indexer par fichier et type
        if entity.filepath:
            if entity_id not in self.file_to_entities[entity.filepath]:
                self.file_to_entities[entity.filepath].append(entity_id)
        if entity_id not in self.type_to_entities[entity.entity_type]:
            self.type_to_entities[entity.entity_type].append(entity_id)

    def update_entity_concepts(self, entity_id: str, new_concepts: List[Dict[str, Any]]):
        """Met √† jour (ajoute) les concepts d√©tect√©s pour une entit√© sp√©cifique."""
        if entity_id in self.entities:
            entity = self.entities[entity_id]
            existing_uris = {c.get('concept_uri') for c in entity.detected_concepts if c.get('concept_uri')}
            for concept in new_concepts:
                if concept.get('concept_uri') not in existing_uris:
                    entity.detected_concepts.append(concept)
                    existing_uris.add(concept.get('concept_uri'))

    def _generate_entity_id(self, entity_info: Dict[str, Any], base_name: str) -> str:
        """G√©n√®re un ID unique pour une entit√©"""
        filepath = entity_info.get('filepath', '')
        entity_type = entity_info.get('type', 'unknown')
        start_line = entity_info.get('start_line', 0)

        # Utiliser parent_entity_id s'il existe (pour les entit√©s splitt√©es)
        parent_id = entity_info.get('parent_entity_id')
        if parent_id:
            return parent_id

        # Sinon cr√©er un ID bas√© sur le contexte
        return f"{filepath}#{entity_type}#{base_name}#{start_line}"

    def _extract_part_index(self, entity_name: str) -> int:
        """Extrait l'index de partie depuis le nom"""
        match = re.search(r'_part_(\d+)$', entity_name)
        return int(match.group(1)) if match else 0

    def _extract_concept_labels(self, concepts: List[Any]) -> Set[str]:
        """Extrait les labels des concepts"""
        labels = set()
        for concept in concepts:
            if isinstance(concept, dict):
                label = concept.get('label', '')
                if label:
                    labels.add(label)
            else:
                labels.add(str(concept))
        return labels

    async def _build_relationships(self):
        """Construit les relations hi√©rarchiques"""
        for entity in self.entities.values():
            if entity.parent_entity:
                parent_entity_id = self.name_to_entity.get(entity.parent_entity.lower())
                if parent_entity_id:
                    self.parent_to_children[parent_entity_id].append(entity.entity_id)
                    self.child_to_parent[entity.entity_id] = parent_entity_id

    async def _detect_and_group_split_entities(self):
        """D√©tecte et regroupe les entit√©s splitt√©es"""
        for entity in self.entities.values():
            if len(entity.chunks) > 1:
                entity.is_grouped = True
                entity.expected_parts = len(entity.chunks)

                # Trier les chunks par partie
                entity.chunks.sort(key=lambda x: x.get('part_index', 0))

                # V√©rifier la compl√©tude
                part_indices = [chunk.get('part_index', 0) for chunk in entity.chunks]
                expected_indices = list(range(1, len(entity.chunks) + 1))
                entity.is_complete = sorted(part_indices) == expected_indices

                logger.debug(f"Entit√© regroup√©e: {entity.entity_name} "
                             f"({len(entity.chunks)} parties, "
                             f"{'compl√®te' if entity.is_complete else 'incompl√®te'})")

    # === M√©thodes de recherche et acc√®s ===

    async def find_entity(self, name: str) -> Optional[UnifiedEntity]:
        """Trouve une entit√© par nom (exact ou fuzzy)"""
        entity_id = self.name_to_entity.get(name.lower())
        if entity_id:
            return self.entities.get(entity_id)

        # Recherche fuzzy
        return await self._fuzzy_search_entity(name)

    async def _fuzzy_search_entity(self, name: str) -> Optional[UnifiedEntity]:
        """Recherche fuzzy d'entit√©"""
        name_lower = name.lower()

        # Chercher des noms similaires
        similar_names = [
            entity_name for entity_name in self.name_to_entity.keys()
            if name_lower in entity_name or entity_name in name_lower
        ]

        if similar_names:
            # Prendre le plus similaire (par longueur)
            best_match = min(similar_names, key=lambda x: abs(len(x) - len(name_lower)))
            entity_id = self.name_to_entity[best_match]
            return self.entities.get(entity_id)

        return None

    async def get_entities_by_type(self, entity_type: str) -> List[UnifiedEntity]:
        """R√©cup√®re les entit√©s par type - VERSION CORRIG√âE POUR √âVITER GENERATORS"""
        if not self._initialized:
            await self.initialize()

        entity_ids = self.type_to_entities.get(entity_type, [])

        # CORRECTION: S'assurer de retourner une liste, pas un generator
        entities = []
        for eid in entity_ids:
            if eid in self.entities:
                entities.append(self.entities[eid])

        return entities  # Liste explicite

    async def get_entities_in_file(self, filepath: str) -> List[UnifiedEntity]:
        """R√©cup√®re les entit√©s dans un fichier - VERSION CORRIG√âE"""
        if not self._initialized:
            await self.initialize()

        entity_ids = self.file_to_entities.get(filepath, [])

        # CORRECTION: Liste explicite
        entities = []
        for eid in entity_ids:
            if eid in self.entities:
                entities.append(self.entities[eid])

        return entities  # Liste explicite

    async def get_children(self, entity_id: str) -> List[UnifiedEntity]:
        """R√©cup√®re les entit√©s enfants - VERSION CORRIG√âE"""
        if not self._initialized:
            await self.initialize()

        child_ids = self.parent_to_children.get(entity_id, [])
        children = []

        for child_id in child_ids:
            if child_id in self.entities:
                children.append(self.entities[child_id])

        return children  # Retourner une liste, pas un generator

    async def get_parent(self, entity_id: str) -> Optional[UnifiedEntity]:
        """R√©cup√®re l'entit√© parent"""
        parent_id = self.child_to_parent.get(entity_id)
        return self.entities.get(parent_id) if parent_id else None

    # === M√©thodes d'analyse des d√©pendances ===

    async def find_entity_callers(self, entity_name: str) -> List[Dict[str, Any]]:
        """Trouve qui appelle une entit√©, avec le num√©ro de ligne EXACT de l'appel."""
        cache_key = f"callers_{entity_name.lower()}"
        cached_result = await global_cache.function_calls.get(cache_key)
        if cached_result:
            return cached_result

        callers = []
        target_name_lower = entity_name.lower()

        for entity in self.entities.values():
            if entity.entity_name.lower() == target_name_lower:
                continue

            if not isinstance(entity.called_functions, list):
                continue

            # ‚ñº‚ñº‚ñº NOUVELLE LOGIQUE D'IT√âRATION PR√âCISE ‚ñº‚ñº‚ñº
            # On parcourt chaque appel DANS l'entit√© pour trouver une correspondance.
            for call_info in entity.called_functions:
                # S√©curit√© : s'assurer que call_info est bien un dictionnaire avec un nom
                if isinstance(call_info, dict) and 'name' in call_info:
                    call_name_lower = call_info['name'].lower()

                    # Si le nom de l'appel correspond √† notre cible...
                    if call_name_lower == target_name_lower:
                        # On a trouv√© un appel ! On r√©cup√®re la ligne de CET appel.
                        line_of_call = call_info.get('line', 0)  # Ligne exacte de l'appel

                        # On ajoute les infos de l'entit√© APPELANTE avec la LIGNE EXACTE.
                        callers.append({
                            'name': entity.entity_name,  # Qui appelle
                            'type': entity.entity_type,
                            'file': entity.filepath,
                            'entity_id': entity.entity_id,
                            'call_line': line_of_call  # <--- INFORMATION PR√âCISE
                        })

        await global_cache.function_calls.set(cache_key, callers, ttl=1800)
        return callers

    async def get_dependency_graph(self, entity_name: str, max_depth: int = 2) -> Dict[str, Any]:
        """Construit un graphe de d√©pendances centr√© sur une entit√©"""
        cache_key = f"depgraph_{entity_name.lower()}_{max_depth}"

        # V√©rifier le cache
        cached_graph = await global_cache.dependency_graphs.get(cache_key)
        if cached_graph:
            return cached_graph

        graph = await self._build_dependency_subgraph(entity_name, max_depth)

        # Mettre en cache
        await global_cache.dependency_graphs.set(cache_key, graph, ttl=3600)  # 1h

        return graph

    async def _build_dependency_subgraph(self, entity_name: str, max_depth: int) -> Dict[str, Any]:
        """Construit un sous-graphe de d√©pendances"""
        entity = await self.find_entity(entity_name)
        if not entity:
            return {}

        visited = set()
        graph = {
            'root_entity': entity_name,
            'nodes': {},
            'edges': [],
            'levels': defaultdict(list)
        }

        # BFS pour explorer les d√©pendances
        queue = [(entity.entity_id, 0)]

        while queue and len(graph['nodes']) < 50:  # Limiter la taille
            entity_id, level = queue.pop(0)

            if entity_id in visited or level > max_depth:
                continue

            visited.add(entity_id)
            current_entity = self.entities.get(entity_id)
            if not current_entity:
                continue

            # Ajouter le n≈ìud
            graph['nodes'][current_entity.entity_name] = {
                'type': current_entity.entity_type,
                'file': current_entity.filepath,
                'level': level,
                'entity_id': entity_id,
                'is_grouped': current_entity.is_grouped
            }

            graph['levels'][level].append(current_entity.entity_name)

            # Explorer les d√©pendances
            for dep_name in current_entity.dependencies:
                dep_entity = await self.find_entity(dep_name)
                if dep_entity and dep_entity.entity_id not in visited:
                    queue.append((dep_entity.entity_id, level + 1))
                    graph['edges'].append({
                        'from': current_entity.entity_name,
                        'to': dep_name,
                        'type': 'uses'
                    })

            # Explorer les appels de fonctions
            for call_name in current_entity.called_functions:
                call_entity = await self.find_entity(call_name)
                if call_entity and call_entity.entity_id not in visited:
                    queue.append((call_entity.entity_id, level + 1))
                    graph['edges'].append({
                        'from': current_entity.entity_name,
                        'to': call_name,
                        'type': 'calls'
                    })

        return graph

    # === M√©thodes utilitaires et statistiques ===

    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du gestionnaire"""
        type_counts = defaultdict(int)
        grouped_count = 0
        incomplete_count = 0
        total_chunks = 0

        for entity in self.entities.values():
            type_counts[entity.entity_type] += 1
            if entity.is_grouped:
                grouped_count += 1
            if not entity.is_complete:
                incomplete_count += 1
            total_chunks += len(entity.chunks)

        return {
            'total_entities': len(self.entities),
            'total_chunks': total_chunks,
            'grouped_entities': grouped_count,
            'incomplete_entities': incomplete_count,
            'entity_types': dict(type_counts),
            'files_indexed': len(self.file_to_entities),
            'compression_ratio': total_chunks / max(1, len(self.entities))
        }

    async def get_stats_async(self) -> Dict[str, Any]:
        """Version async des statistiques"""
        return self.get_stats()  # D√©l√©guer √† la version sync

    async def update_function_calls(self, entity_name: str, called_functions: List[str]):
        """Met √† jour les appels de fonctions d'une entit√©"""
        entity = await self.find_entity(entity_name)
        if entity:
            entity.called_functions = set(called_functions)

            # Invalider les caches li√©s
            cache_keys_to_clear = [
                f"callers_{entity_name.lower()}",
                f"depgraph_{entity_name.lower()}_2"
            ]

            for cache_key in cache_keys_to_clear:
                await global_cache.function_calls.delete(cache_key)
                await global_cache.dependency_graphs.delete(cache_key)

    def clear_caches(self):
        """Vide les caches (utile pour tests)"""
        self._search_cache.clear()

    async def rebuild_index(self):
        """Reconstruit l'index complet"""
        logger.info("üîÑ Reconstruction de l'index des entit√©s...")

        # R√©initialiser les structures
        self.entities.clear()
        self.name_to_entity.clear()
        self.file_to_entities.clear()
        self.type_to_entities.clear()
        self.parent_to_children.clear()
        self.child_to_parent.clear()

        # Reconstruire
        await self._build_unified_index()
        await self._build_relationships()
        await self._detect_and_group_split_entities()

        logger.info(f"‚úÖ Index reconstruit: {len(self.entities)} entit√©s")


# Instance globale pour utilisation dans le syst√®me
_global_entity_manager = None


async def get_entity_manager(document_store) -> EntityManager:
    """Factory pour obtenir le gestionnaire d'entit√©s global"""
    global _global_entity_manager
    if _global_entity_manager is None:
        _global_entity_manager = EntityManager(document_store)
        await _global_entity_manager.initialize()
    return _global_entity_manager
