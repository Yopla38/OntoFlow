"""
    ------------------------------------------
    Copyright: CEA Grenoble
    Auteur: Yoann CURE
    Entit√©: IRIG
    Ann√©e: 2025
    Description: Agent IA d'Int√©gration Continue
    ------------------------------------------
    """

import os
import re
import hashlib
from typing import List, Dict, Any, Tuple, Optional, Set
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import asyncio

import numpy as np


@dataclass
class DocumentHierarchy:
    """Hi√©rarchie conceptuelle du document"""
    document_concepts: List[Dict[str, Any]] = field(default_factory=list)
    section_concepts: Dict[str, List[Dict[str, Any]]] = field(default_factory=dict)
    paragraph_concepts: Dict[str, List[Dict[str, Any]]] = field(default_factory=dict)


@dataclass
class SemanticChunk:
    """Chunk s√©mantique avec ses concepts"""
    chunk_id: str
    text: str
    start_pos: int
    end_pos: int
    level: str  # 'document', 'section', 'paragraph'
    concepts: List[Dict[str, Any]] = field(default_factory=list)
    inherited_concepts: List[Dict[str, Any]] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


class GenericContextResolver:
    """R√©solveur de contexte g√©n√©rique bas√© sur l'ontologie"""

    def __init__(self, ontology_manager, concept_classifier):
        self.ontology_manager = ontology_manager
        self.concept_classifier = concept_classifier

        # Cache des compatibilit√©s calcul√©es
        self.compatibility_cache = {}
        self.concept_embeddings_cache = {}

    def _extract_document_context(self, document_concepts: List[Dict[str, Any]]) -> Dict[str, float]:
        """Extrait le contexte de fa√ßon g√©n√©rique via clustering de concepts"""

        if not document_concepts:
            return {}

        # 1. R√©cup√©rer les embeddings des concepts dominants
        concept_embeddings = []
        concept_info = []

        for concept in document_concepts[:10]:  # Top 10
            uri = concept.get('concept_uri', '')
            embedding = self._get_concept_embedding(uri)
            if embedding is not None:
                concept_embeddings.append(embedding)
                concept_info.append(concept)

        if not concept_embeddings:
            return {}

        # 2. Clustering automatique pour identifier les groupes s√©mantiques
        context_clusters = self._cluster_concepts(concept_embeddings, concept_info)

        # 3. Calculer les poids des clusters
        context_weights = {}
        total_confidence = sum(c['confidence'] for c in document_concepts[:10])

        for cluster_id, cluster_info in context_clusters.items():
            cluster_weight = sum(c['confidence'] for c in cluster_info['concepts']) / total_confidence
            context_weights[cluster_id] = cluster_weight

        print(f"   üß† Contexte g√©n√©rique: {len(context_clusters)} clusters d√©tect√©s")
        for cluster_id, weight in context_weights.items():
            concepts_labels = [c['label'] for c in context_clusters[cluster_id]['concepts'][:3]]
            print(f"      - {cluster_id}: {weight:.2f} ({', '.join(concepts_labels)})")

        return context_weights, context_clusters

    def _cluster_concepts(self, embeddings: List[np.ndarray], concept_info: List[Dict]) -> Dict[str, Dict]:
        """Clustering automatique des concepts par similarit√© s√©mantique"""

        if len(embeddings) < 2:
            return {"cluster_0": {"concepts": concept_info, "centroid": embeddings[0] if embeddings else None}}

        # Clustering simple par seuil de similarit√©
        clusters = {}
        cluster_id = 0

        for i, (embedding, concept) in enumerate(zip(embeddings, concept_info)):
            assigned = False

            # Tenter d'assigner √† un cluster existant
            for cid, cluster_data in clusters.items():
                centroid = cluster_data['centroid']
                similarity = np.dot(embedding, centroid)

                if similarity > 0.7:  # Seuil de similarit√©
                    cluster_data['concepts'].append(concept)
                    # Recalculer le centro√Øde
                    all_embeddings = [self._get_concept_embedding(c['concept_uri']) for c in cluster_data['concepts']]
                    cluster_data['centroid'] = np.mean([e for e in all_embeddings if e is not None], axis=0)
                    assigned = True
                    break

            # Cr√©er un nouveau cluster si pas assign√©
            if not assigned:
                cluster_name = f"cluster_{cluster_id}"
                clusters[cluster_name] = {
                    "concepts": [concept],
                    "centroid": embedding
                }
                cluster_id += 1

        return clusters

    def _get_concept_context_score(self, concept: Dict[str, Any],
                                   context_clusters: Dict[str, Dict]) -> float:
        """Calcule le score de contexte de fa√ßon g√©n√©rique"""

        concept_uri = concept.get('concept_uri', '')
        concept_embedding = self._get_concept_embedding(concept_uri)

        if concept_embedding is None:
            return 0.0

        max_compatibility = 0.0
        best_cluster = None

        # Calculer la compatibilit√© avec chaque cluster de contexte
        for cluster_id, cluster_data in context_clusters.items():
            cluster_centroid = cluster_data['centroid']

            # Similarit√© s√©mantique directe
            semantic_similarity = np.dot(concept_embedding, cluster_centroid)

            # Compatibilit√© ontologique
            ontology_compatibility = self._calculate_ontology_compatibility(
                concept_uri, [c['concept_uri'] for c in cluster_data['concepts']]
            )

            # Score combin√©
            total_compatibility = (semantic_similarity * 0.6) + (ontology_compatibility * 0.4)

            if total_compatibility > max_compatibility:
                max_compatibility = total_compatibility
                best_cluster = cluster_id

        print(f"     üéØ {concept['label']} ‚Üí {best_cluster} (compat: {max_compatibility:.2f})")
        return max_compatibility

    def _calculate_ontology_compatibility(self, concept_uri: str, cluster_concept_uris: List[str]) -> float:
        """Calcule la compatibilit√© ontologique g√©n√©rique"""

        if not cluster_concept_uris:
            return 0.0

        # Cache key
        cache_key = f"{concept_uri}::{':'.join(sorted(cluster_concept_uris))}"
        if cache_key in self.compatibility_cache:
            return self.compatibility_cache[cache_key]

        max_compatibility = 0.0

        for cluster_uri in cluster_concept_uris:
            compatibility = self._calculate_pairwise_compatibility(concept_uri, cluster_uri)
            max_compatibility = max(max_compatibility, compatibility)

        # Cache du r√©sultat
        self.compatibility_cache[cache_key] = max_compatibility
        return max_compatibility

    def _calculate_pairwise_compatibility(self, uri1: str, uri2: str) -> float:
        """Calcule la compatibilit√© entre deux concepts via l'ontologie"""

        if uri1 == uri2:
            return 1.0

        # 1. Relations hi√©rarchiques directes
        if self._are_hierarchically_related(uri1, uri2):
            return 0.8

        # 2. Relations s√©mantiques dans l'ontologie
        semantic_relation = self._get_semantic_relation(uri1, uri2)
        if semantic_relation:
            return self._relation_to_score(semantic_relation)

        # 3. M√™me domaine ontologique
        if self._same_ontology_domain(uri1, uri2):
            return 0.4

        # 4. Co-occurrence apprise (si disponible)
        cooccurrence = self._get_learned_cooccurrence(uri1, uri2)
        if cooccurrence > 0:
            return cooccurrence * 0.6

        return 0.0

    def _are_hierarchically_related(self, uri1: str, uri2: str) -> bool:
        """V√©rifie les relations hi√©rarchiques"""

        concept1 = self.ontology_manager.concepts.get(uri1)
        concept2 = self.ontology_manager.concepts.get(uri2)

        if not concept1 or not concept2:
            return False

        # Parent-enfant direct
        if hasattr(concept1, 'parents'):
            for parent in concept1.parents:
                if hasattr(parent, 'uri') and parent.uri == uri2:
                    return True

        if hasattr(concept2, 'parents'):
            for parent in concept2.parents:
                if hasattr(parent, 'uri') and parent.uri == uri1:
                    return True

        return False

    def _get_semantic_relation(self, uri1: str, uri2: str) -> Optional[str]:
        """R√©cup√®re la relation s√©mantique entre deux concepts"""

        for axiom_type, source, target in self.ontology_manager.axioms:
            if (source == uri1 and target == uri2) or (source == uri2 and target == uri1):
                return axiom_type

        return None

    def _relation_to_score(self, relation_type: str) -> float:
        """Convertit un type de relation en score de compatibilit√©"""

        relation_scores = {
            'semantic_equivalent': 0.9,
            'semantic_similar': 0.7,
            'semantic_related': 0.5,
            'semantic_opposite': -0.8,  # Incompatible !
            'semantic_exclusive': -0.9  # Tr√®s incompatible !
        }

        return relation_scores.get(relation_type, 0.3)

    def _same_ontology_domain(self, uri1: str, uri2: str) -> bool:
        """V√©rifie si deux concepts sont du m√™me domaine ontologique"""

        if '#' not in uri1 or '#' not in uri2:
            return False

        domain1 = uri1.rsplit('#', 1)[0]
        domain2 = uri2.rsplit('#', 1)[0]

        return domain1 == domain2

    def _get_learned_cooccurrence(self, uri1: str, uri2: str) -> float:
        """Score de co-occurrence appris (√† impl√©menter si apprentissage disponible)"""

        # TODO: Impl√©menter avec les donn√©es d'apprentissage
        # Analyser les documents o√π les deux concepts apparaissent ensemble
        return 0.0

    def _get_concept_embedding(self, concept_uri: str) -> Optional[np.ndarray]:
        """R√©cup√®re l'embedding d'un concept avec cache"""

        if concept_uri in self.concept_embeddings_cache:
            return self.concept_embeddings_cache[concept_uri]

        embedding = None
        if (hasattr(self.concept_classifier, 'concept_embeddings') and
                concept_uri in self.concept_classifier.concept_embeddings):
            embedding = self.concept_classifier.concept_embeddings[concept_uri]

        self.concept_embeddings_cache[concept_uri] = embedding
        return embedding

class SelectiveConceptDetector:
    """D√©tection s√©lective de concepts selon le niveau hi√©rarchique"""

    def __init__(self, concept_classifier, ontology_manager):
        self.concept_classifier = concept_classifier
        self.ontology_manager = ontology_manager

    async def detect_document_level_concepts(self, text: str, max_concepts: int = 5) -> List[Dict[str, Any]]:
        """D√©tection S√âLECTIVE au niveau document - seulement les concepts dominants"""

        print(f"üß† D√©tection s√©lective au niveau document (max {max_concepts} concepts)")

        # D√©tecter tous les concepts
        all_concepts = await self.concept_classifier.smart_concept_detection(text)

        # NOUVEAU : Filtrage par dominance contextuelle
        dominant_concepts = self._filter_dominant_concepts(all_concepts, text, max_concepts)

        print(f"   üìã {len(dominant_concepts)} concepts dominants s√©lectionn√©s:")
        for concept in dominant_concepts:
            print(
                f"      - {concept['label']} (conf: {concept['confidence']:.2f}, dominance: {concept.get('dominance_score', 0):.2f})")

        return dominant_concepts

    def _filter_dominant_concepts(self, concepts: List[Dict[str, Any]], text: str, max_concepts: int) -> List[
        Dict[str, Any]]:
        """Filtre les concepts dominants selon le contexte"""

        # 1. Calculer la dominance contextuelle
        for concept in concepts:
            concept['dominance_score'] = self._calculate_dominance_score(concept, text)

        # 2. R√©soudre les ambigu√Øt√©s AVANT la s√©lection
        resolved_concepts = self._resolve_ambiguities_early(concepts)

        # 3. S√©lectionner les plus dominants
        resolved_concepts.sort(key=lambda x: x['dominance_score'], reverse=True)

        return resolved_concepts[:max_concepts]

    def _calculate_dominance_score(self, concept: Dict[str, Any], text: str) -> float:
        """Calcule le score de dominance d'un concept dans le texte"""
        label = concept.get('label', '').lower()
        uri = concept.get('concept_uri', '').lower()
        base_confidence = concept.get('confidence', 0)

        # Compter les occurrences dans le texte
        text_lower = text.lower()
        label_occurrences = text_lower.count(label)

        # Bonus pour les concepts qui apparaissent plusieurs fois
        frequency_bonus = min(0.3, label_occurrences * 0.1)

        # Bonus pour les concepts en d√©but de document
        position_bonus = 0.1 if label in text_lower[:200] else 0

        # Malus pour les concepts tr√®s g√©n√©riques
        generic_penalty = 0.1 if label in ['domaine physique', 'physics'] else 0

        dominance = base_confidence + frequency_bonus + position_bonus - generic_penalty

        return dominance

    def _resolve_ambiguities_early(self, concepts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """R√©sout les ambigu√Øt√©s AVANT la s√©lection des concepts dominants"""

        # Grouper par label pour identifier les ambigu√Øt√©s
        grouped = {}
        for concept in concepts:
            label = concept.get('label', '').lower()
            if label not in grouped:
                grouped[label] = []
            grouped[label].append(concept)

        resolved = []

        for label, concept_group in grouped.items():
            if len(concept_group) == 1:
                resolved.append(concept_group[0])
            else:
                # Ambigu√Øt√© : prendre le plus dominant
                print(f"   ‚ö†Ô∏è  Ambigu√Øt√© pr√©coce sur '{label}' - {len(concept_group)} variantes")

                best_concept = max(concept_group, key=lambda x: x['dominance_score'])
                best_concept['early_ambiguity_resolved'] = True
                resolved.append(best_concept)

                chosen = best_concept.get('concept_uri', '').split('#')[-1]
                print(f"   ‚úÖ Choix pr√©coce: {chosen} (dominance: {best_concept['dominance_score']:.2f})")

        return resolved

    async def detect_section_level_concepts(self, text: str, document_concepts: List[Dict[str, Any]],
                                            max_concepts: int = 8) -> List[Dict[str, Any]]:
        """D√©tection au niveau section - dans le voisinage des concepts document"""

        print(f"üîç D√©tection au niveau section (max {max_concepts} concepts)")

        # Construire l'espace de recherche depuis les concepts document
        search_space = self._build_section_search_space(document_concepts)

        # D√©tection globale
        all_detected = await self.concept_classifier.smart_concept_detection(text)

        # Filtrer par l'espace de recherche
        section_concepts = []
        for concept in all_detected:
            concept_uri = concept.get('concept_uri', '')
            if concept_uri in search_space:
                # Calculer la vraie distance depuis les concepts document
                distance = self._calculate_real_distance(concept_uri, document_concepts)
                concept['path_distance'] = distance
                concept['in_section_path'] = True
                section_concepts.append(concept)

        # Limiter et trier
        section_concepts.sort(key=lambda x: (x['path_distance'], -x['confidence']))
        return section_concepts[:max_concepts]

    def _build_section_search_space(self, document_concepts: List[Dict[str, Any]]) -> Set[str]:
        """Construit l'espace de recherche pour les sections"""
        search_space = set()

        for concept in document_concepts:
            concept_uri = concept.get('concept_uri', '')
            if concept_uri:
                # Concept lui-m√™me
                search_space.add(concept_uri)

                # Voisins directs seulement (distance 1)
                neighbors = self._get_direct_neighbors(concept_uri)
                search_space.update(neighbors)

        return search_space

    def _get_direct_neighbors(self, concept_uri: str) -> Set[str]:
        """R√©cup√®re seulement les voisins directs (distance 1)"""
        neighbors = set()

        # Relations hi√©rarchiques directes
        concept = self.ontology_manager.concepts.get(concept_uri)
        if concept:
            if hasattr(concept, 'parents') and concept.parents:
                for parent in concept.parents:
                    if hasattr(parent, 'uri'):
                        neighbors.add(parent.uri)

            if hasattr(concept, 'children') and concept.children:
                for child in concept.children:
                    if hasattr(child, 'uri'):
                        neighbors.add(child.uri)

        # Relations s√©mantiques directes
        for axiom_type, source, target in self.ontology_manager.axioms:
            if source == concept_uri:
                neighbors.add(target)
            elif target == concept_uri:
                neighbors.add(source)

        return neighbors

    def _calculate_real_distance(self, concept_uri: str, document_concepts: List[Dict[str, Any]]) -> int:
        """Calcule la vraie distance dans l'ontologie"""
        min_distance = 999

        for doc_concept in document_concepts:
            doc_uri = doc_concept.get('concept_uri', '')
            if doc_uri == concept_uri:
                return 0

            # V√©rifier si c'est un voisin direct
            doc_neighbors = self._get_direct_neighbors(doc_uri)
            if concept_uri in doc_neighbors:
                min_distance = min(min_distance, 1)

        return min_distance if min_distance < 999 else 2


class LevelBasedSearchEngine:
    """Moteur de recherche par niveau hi√©rarchique"""

    def __init__(self, document_store, embedding_provider):
        self.document_store = document_store
        self.embedding_provider = embedding_provider

    async def search_by_level(self, query: str, level: str = 'document',
                              top_k: int = 5) -> List[Dict[str, Any]]:
        """Recherche par niveau sp√©cifique"""

        print(f"üîç Recherche niveau '{level}' pour: {query}")

        # G√©n√©rer l'embedding de la requ√™te
        query_embedding = await self.embedding_provider.generate_embeddings([query])
        query_embedding = query_embedding[0]

        # Collecter les chunks du niveau demand√©
        level_chunks = []

        for doc_id, chunks in self.document_store.document_chunks.items():
            for chunk in chunks:
                chunk_level = chunk.get('metadata', {}).get('chunk_level', 'unknown')
                if chunk_level == level:
                    level_chunks.append(chunk)

        if not level_chunks:
            print(f"   ‚ö†Ô∏è  Aucun chunk de niveau '{level}' trouv√©")
            return []

        # Calculer les similarit√©s
        similarities = []
        for chunk in level_chunks:
            chunk_id = chunk['id']
            chunk_embedding = self.document_store.embedding_manager.get_embedding(chunk_id)

            if chunk_embedding is not None:
                similarity = float(np.dot(query_embedding, chunk_embedding))

                # CORRECTION : Enrichir avec les m√©tadonn√©es de source
                metadata = chunk.get('metadata', {})
                result = {
                    'chunk': chunk,
                    'similarity': similarity,
                    'source_info': {
                        'filename': metadata.get('filename', 'Fichier inconnu'),
                        'filepath': metadata.get('filepath', ''),
                        'start_line': metadata.get('start_pos', 0),
                        'end_line': metadata.get('end_pos', 0),
                        'section_title': metadata.get('section_title', 'Section sans titre'),
                        'chunk_level': metadata.get('chunk_level', level),
                        'document_id': chunk.get('document_id', 'unknown')
                    }
                }
                similarities.append(result)

        # Trier et limiter
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        top_results = similarities[:top_k]

        print(f"   ‚úÖ {len(top_results)} r√©sultats trouv√©s au niveau '{level}'")

        return top_results

    async def hierarchical_search(self, query: str, max_per_level: int = 3) -> Dict[str, List[Dict[str, Any]]]:
        """Recherche hi√©rarchique sur tous les niveaux"""

        print(f"üîç Recherche hi√©rarchique pour: {query}")

        results = {}

        # Rechercher √† chaque niveau
        for level in ['document', 'section', 'paragraph']:
            level_results = await self.search_by_level(query, level, max_per_level)
            if level_results:
                results[level] = level_results

                print(f"   üìã Niveau {level}: {len(level_results)} r√©sultats")
                for result in level_results[:2]:  # Top 2
                    chunk = result['chunk']
                    title = chunk.get('metadata', {}).get('section_title', 'Sans titre')
                    print(f"      - {title} (sim: {result['similarity']:.2f})")

        return results


class OntologyPathNavigator:
    """Navigue dans l'ontologie en suivant un chemin conceptuel"""

    def __init__(self, ontology_manager, concept_classifier):
        self.ontology_manager = ontology_manager
        self.concept_classifier = concept_classifier

    def build_search_space(self, inherited_concepts: List[Dict[str, Any]]) -> Set[str]:
        """Construit l'espace de recherche bas√© sur les concepts h√©rit√©s - VERSION CORRIG√âE"""
        search_space = set()

        print(f"   üõ§Ô∏è  Construction de l'espace de recherche depuis {len(inherited_concepts)} concepts h√©rit√©s")

        for concept in inherited_concepts:
            concept_uri = concept.get('concept_uri', '')
            concept_label = concept.get('label', '')

            if not concept_uri:
                continue

            # Ajouter le concept lui-m√™me
            search_space.add(concept_uri)

            # Ajouter ses voisins AVEC contr√¥le de distance
            neighbors = self.get_concept_neighbors(concept_uri, max_distance=2)

            # CORRECTION : Filtrer les voisins par pertinence
            relevant_neighbors = set()
            for neighbor_uri in neighbors:
                distance = self._calculate_ontology_distance(concept_uri, neighbor_uri)
                if distance <= 2:  # Seulement distance 1 et 2
                    relevant_neighbors.add(neighbor_uri)

            search_space.update(relevant_neighbors)

            print(f"     - {concept_label} ‚Üí {len(relevant_neighbors)} voisins pertinents ajout√©s")

        print(f"   üîç Espace de recherche final: {len(search_space)} concepts possibles")
        return search_space

    async def detect_concepts_in_path(self, text: str, inherited_concepts: List[Dict[str, Any]]) -> List[
        Dict[str, Any]]:
        """D√©tecte les concepts en restant dans le chemin ontologique"""

        if not inherited_concepts:
            # Pas de concepts h√©rit√©s = d√©tection libre (niveau document)
            print(f"   üÜì D√©tection libre (pas de concepts h√©rit√©s)")
            return await self.concept_classifier.smart_concept_detection(text)

        # Construire l'espace de recherche
        search_space = self.build_search_space(inherited_concepts)

        # D√©tection globale
        all_detected = await self.concept_classifier.smart_concept_detection(text)

        # Filtrer par l'espace de recherche
        filtered_concepts = []
        for concept in all_detected:
            concept_uri = concept.get('concept_uri', '')

            if concept_uri in search_space:
                concept['in_ontology_path'] = True
                concept['path_distance'] = self._calculate_path_distance(concept_uri, inherited_concepts)
                filtered_concepts.append(concept)
                print(f"     ‚úÖ Gard√©: {concept['label']} (dans le chemin, distance: {concept['path_distance']})")
            else:
                print(f"     ‚ùå √âlimin√©: {concept['label']} (hors chemin ontologique)")

        print(f"   ‚úÖ {len(filtered_concepts)} concepts retenus dans le chemin")
        return filtered_concepts

    def _calculate_path_distance(self, concept_uri: str, inherited_concepts: List[Dict[str, Any]]) -> int:
        """Calcule la distance minimale dans le chemin ontologique - VERSION CORRIG√âE"""
        min_distance = 999

        print(f"     üîç Calcul distance pour {concept_uri.split('#')[-1] if '#' in concept_uri else concept_uri}")

        for inherited in inherited_concepts:
            inherited_uri = inherited.get('concept_uri', '')
            inherited_label = inherited.get('label', '')

            if inherited_uri == concept_uri:
                print(f"       ‚Üí Distance 0 (m√™me concept que {inherited_label})")
                return 0  # M√™me concept

            # CORRECTION 1 : V√©rifier les relations directes dans l'ontologie
            distance = self._calculate_ontology_distance(concept_uri, inherited_uri)
            if distance < min_distance:
                min_distance = distance
                print(f"       ‚Üí Distance {distance} via {inherited_label}")

        final_distance = min_distance if min_distance < 999 else 3
        print(f"       ‚úÖ Distance finale: {final_distance}")
        return final_distance

    def _calculate_ontology_distance(self, concept_uri: str, inherited_uri: str) -> int:
        """Calcule la distance r√©elle dans l'ontologie entre deux concepts"""

        # Distance 1 : Relations directes
        if self._are_directly_related(concept_uri, inherited_uri):
            return 1

        # Distance 2 : Relations via un interm√©diaire
        if self._are_related_via_intermediate(concept_uri, inherited_uri):
            return 2

        # Distance 3 : M√™me domaine
        if self._are_same_domain(concept_uri, inherited_uri):
            return 3

        return 999  # Non li√©

    def _are_directly_related(self, concept_uri1: str, concept_uri2: str) -> bool:
        """V√©rifie si deux concepts sont directement li√©s"""

        # 1. Relations hi√©rarchiques (parent/enfant)
        concept1 = self.ontology_manager.concepts.get(concept_uri1)
        concept2 = self.ontology_manager.concepts.get(concept_uri2)

        if concept1 and concept2:
            # V√©rifier si concept1 est parent de concept2
            if hasattr(concept1, 'children'):
                for child in concept1.children:
                    if hasattr(child, 'uri') and child.uri == concept_uri2:
                        return True

            # V√©rifier si concept2 est parent de concept1
            if hasattr(concept2, 'children'):
                for child in concept2.children:
                    if hasattr(child, 'uri') and child.uri == concept_uri1:
                        return True

            # V√©rifier si concept1 est enfant de concept2
            if hasattr(concept1, 'parents'):
                for parent in concept1.parents:
                    if hasattr(parent, 'uri') and parent.uri == concept_uri2:
                        return True

            # V√©rifier si concept2 est enfant de concept1
            if hasattr(concept2, 'parents'):
                for parent in concept2.parents:
                    if hasattr(parent, 'uri') and parent.uri == concept_uri1:
                        return True

        # 2. Relations s√©mantiques d√©finies dans l'ontologie
        for axiom_type, source, target in self.ontology_manager.axioms:
            if (source == concept_uri1 and target == concept_uri2) or \
                    (source == concept_uri2 and target == concept_uri1):
                return True

        return False

    def _are_related_via_intermediate(self, concept_uri1: str, concept_uri2: str) -> bool:
        """V√©rifie si deux concepts sont li√©s via un interm√©diaire"""

        # R√©cup√©rer les voisins directs de concept1
        neighbors1 = self.get_concept_neighbors(concept_uri1, max_distance=1)

        # R√©cup√©rer les voisins directs de concept2
        neighbors2 = self.get_concept_neighbors(concept_uri2, max_distance=1)

        # V√©rifier s'ils ont des voisins communs
        common_neighbors = neighbors1.intersection(neighbors2)

        return len(common_neighbors) > 0

    def _are_same_domain(self, concept_uri1: str, concept_uri2: str) -> bool:
        """V√©rifie si deux concepts appartiennent au m√™me domaine"""

        if '#' not in concept_uri1 or '#' not in concept_uri2:
            return False

        domain1 = concept_uri1.rsplit('#', 1)[0]
        domain2 = concept_uri2.rsplit('#', 1)[0]

        return domain1 == domain2

    def get_concept_neighbors(self, concept_uri: str, max_distance: int = 2) -> Set[str]:
        """R√©cup√®re les concepts voisins - VERSION AM√âLIOR√âE"""
        neighbors = set()

        # 1. Relations hi√©rarchiques directes
        concept = self.ontology_manager.concepts.get(concept_uri)
        if concept:
            # Parents
            if hasattr(concept, 'parents') and concept.parents:
                for parent in concept.parents:
                    if hasattr(parent, 'uri'):
                        neighbors.add(parent.uri)

            # Enfants
            if hasattr(concept, 'children') and concept.children:
                for child in concept.children:
                    if hasattr(child, 'uri'):
                        neighbors.add(child.uri)

        # 2. Relations s√©mantiques
        for axiom_type, source, target in self.ontology_manager.axioms:
            if source == concept_uri:
                neighbors.add(target)
            elif target == concept_uri:
                neighbors.add(source)

        # 3. CORRECTION : Concepts du m√™me domaine avec filtrage
        domain = concept_uri.rsplit('#', 1)[0] if '#' in concept_uri else concept_uri
        for uri in self.ontology_manager.concepts.keys():
            if uri.startswith(domain) and uri != concept_uri:
                # NOUVEAU : Limiter aux concepts s√©mantiquement proches
                if self._are_semantically_close(concept_uri, uri):
                    neighbors.add(uri)

        return neighbors

    def _are_semantically_close(self, concept_uri1: str, concept_uri2: str) -> bool:
        """V√©rifie si deux concepts sont s√©mantiquement proches"""

        # R√©cup√©rer les labels
        concept1 = self.ontology_manager.concepts.get(concept_uri1)
        concept2 = self.ontology_manager.concepts.get(concept_uri2)

        if not concept1 or not concept2:
            return False

        label1 = concept1.label.lower() if hasattr(concept1, 'label') else ''
        label2 = concept2.label.lower() if hasattr(concept2, 'label') else ''

        # R√®gles de proximit√© s√©mantique
        proximity_rules = [
            # Distance et longueur d'onde ne sont pas proches
            (['distance', 'meter'], ['wavelength', 'longueur']),
            # M√©canique et optique ne sont pas proches
            (['mechanical', 'm√©canique'], ['optical', 'optique']),
            # Mais les unit√©s du m√™me type sont proches
            (['unit', 'unit√©'], ['unit', 'unit√©']),
        ]

        for group1, group2 in proximity_rules:
            in_group1 = any(keyword in label1 for keyword in group1)
            in_group2 = any(keyword in label2 for keyword in group2)

            if in_group1 and in_group2:
                return False  # Concepts oppos√©s

        # Par d√©faut, les concepts du m√™me domaine sont proches
        return True


class ConceptualPathResolver:
    """R√©solveur d'ambigu√Øt√©s g√©n√©rique bas√© sur la structure ontologique"""

    def __init__(self, document_concepts: List[Dict[str, Any]], ontology_manager):
        self.document_concepts = document_concepts
        self.ontology_manager = ontology_manager

        # Analyser automatiquement le contexte depuis l'ontologie
        self.document_context = self._extract_ontological_context()
        self.concept_domains = self._build_concept_domain_map()
        self.compatibility_matrix = self._build_compatibility_matrix()

    def _extract_ontological_context(self) -> Dict[str, float]:
        """Extrait le contexte automatiquement depuis la structure ontologique"""

        context_weights = {}

        print(f"üß† Analyse ontologique du contexte document ({len(self.document_concepts)} concepts)")

        for concept in self.document_concepts:
            concept_uri = concept.get('concept_uri', '')
            confidence = concept.get('confidence', 0)

            if not concept_uri:
                continue

            # 1. Extraire le domaine ontologique
            domain = self._extract_concept_domain(concept_uri)
            if domain:
                context_weights[domain] = context_weights.get(domain, 0) + confidence

            # 2. Analyser les parents pour comprendre la hi√©rarchie
            parent_domains = self._get_parent_domains(concept_uri)
            for parent_domain in parent_domains:
                # Poids r√©duit pour les parents
                context_weights[parent_domain] = context_weights.get(parent_domain, 0) + (confidence * 0.5)

            # 3. Analyser les axiomes pour les relations s√©mantiques
            semantic_domains = self._get_semantic_domains(concept_uri)
            for sem_domain in semantic_domains:
                context_weights[sem_domain] = context_weights.get(sem_domain, 0) + (confidence * 0.3)

        # Normaliser
        total_weight = sum(context_weights.values())
        if total_weight > 0:
            context_weights = {k: v / total_weight for k, v in context_weights.items()}

        print(f"‚úÖ Contexte ontologique extrait: {context_weights}")
        return context_weights

    def _extract_concept_domain(self, concept_uri: str) -> Optional[str]:
        """Extrait le domaine d'un concept depuis l'ontologie"""

        # 1. Domaine depuis l'URI (namespace)
        if '#' in concept_uri:
            base_uri = concept_uri.rsplit('#', 1)[0]
            domain_name = base_uri.split('/')[-1]
            return domain_name

        # 2. Domaine depuis les m√©tadonn√©es RDF
        concept = self.ontology_manager.concepts.get(concept_uri)
        if concept and hasattr(concept, 'domain'):
            return concept.domain

        # 3. Domaine depuis les parents
        parent_domains = self._get_parent_domains(concept_uri)
        if parent_domains:
            return parent_domains[0]  # Le plus proche

        return None

    def _get_parent_domains(self, concept_uri: str) -> List[str]:
        """R√©cup√®re les domaines des concepts parents"""
        domains = []

        concept = self.ontology_manager.concepts.get(concept_uri)
        if concept and hasattr(concept, 'parents'):
            for parent in concept.parents:
                if hasattr(parent, 'uri'):
                    parent_domain = self._extract_concept_domain(parent.uri)
                    if parent_domain and parent_domain not in domains:
                        domains.append(parent_domain)

        return domains

    def _get_semantic_domains(self, concept_uri: str) -> List[str]:
        """R√©cup√®re les domaines via les relations s√©mantiques"""
        domains = []

        for axiom_type, source, target in self.ontology_manager.axioms:
            related_uri = None

            if source == concept_uri:
                related_uri = target
            elif target == concept_uri:
                related_uri = source

            if related_uri:
                related_domain = self._extract_concept_domain(related_uri)
                if related_domain and related_domain not in domains:
                    domains.append(related_domain)

        return domains

    def _build_concept_domain_map(self) -> Dict[str, str]:
        """Construit une map concept_uri -> domaine principal"""
        concept_domain_map = {}

        for concept_uri in self.ontology_manager.concepts.keys():
            domain = self._extract_concept_domain(concept_uri)
            if domain:
                concept_domain_map[concept_uri] = domain

        return concept_domain_map

    def _build_compatibility_matrix(self) -> Dict[Tuple[str, str], float]:
        """Construit une matrice de compatibilit√© entre domaines"""
        compatibility = {}

        all_domains = set(self.concept_domains.values())

        for domain1 in all_domains:
            for domain2 in all_domains:
                if domain1 == domain2:
                    compatibility[(domain1, domain2)] = 1.0  # Compatible avec soi-m√™me
                else:
                    # Calculer la compatibilit√© bas√©e sur l'ontologie
                    comp_score = self._calculate_domain_compatibility(domain1, domain2)
                    compatibility[(domain1, domain2)] = comp_score

        print(f"üîó Matrice de compatibilit√© construite: {len(compatibility)} relations")
        return compatibility

    def _calculate_domain_compatibility(self, domain1: str, domain2: str) -> float:
        """Calcule la compatibilit√© entre deux domaines"""

        # 1. Compter les concepts partag√©s
        domain1_concepts = {uri for uri, domain in self.concept_domains.items() if domain == domain1}
        domain2_concepts = {uri for uri, domain in self.concept_domains.items() if domain == domain2}

        # 2. Relations hi√©rarchiques entre domaines
        hierarchical_score = self._calculate_hierarchical_compatibility(domain1_concepts, domain2_concepts)

        # 3. Relations s√©mantiques entre domaines
        semantic_score = self._calculate_semantic_compatibility(domain1_concepts, domain2_concepts)

        # 4. Score final
        compatibility = (hierarchical_score * 0.6) + (semantic_score * 0.4)

        # 5. Bonus/malus selon patterns connus (optionnel, peut √™tre supprim√©)
        pattern_bonus = self._calculate_pattern_compatibility(domain1, domain2)
        compatibility += pattern_bonus

        return max(0.0, min(1.0, compatibility))  # Borner entre 0 et 1

    def _calculate_hierarchical_compatibility(self, concepts1: Set[str], concepts2: Set[str]) -> float:
        """Compatibilit√© bas√©e sur les relations hi√©rarchiques"""
        if not concepts1 or not concepts2:
            return 0.0

        relation_count = 0
        total_checks = 0

        for concept1 in concepts1:
            for concept2 in concepts2:
                total_checks += 1

                if self._are_hierarchically_related(concept1, concept2):
                    relation_count += 1

        return relation_count / total_checks if total_checks > 0 else 0.0

    def _calculate_semantic_compatibility(self, concepts1: Set[str], concepts2: Set[str]) -> float:
        """Compatibilit√© bas√©e sur les relations s√©mantiques"""
        if not concepts1 or not concepts2:
            return 0.0

        relation_count = 0
        total_checks = 0

        for concept1 in concepts1:
            for concept2 in concepts2:
                total_checks += 1

                if self._are_semantically_related(concept1, concept2):
                    relation_count += 1

        return relation_count / total_checks if total_checks > 0 else 0.0

    def _calculate_pattern_compatibility(self, domain1: str, domain2: str) -> float:
        """Bonus/malus bas√© sur des patterns optionnels (peut √™tre supprim√©)"""

        # Cette section est optionnelle et peut √™tre supprim√©e pour une approche 100% g√©n√©rique
        # Je la garde juste pour montrer comment on pourrait ajouter des r√®gles sp√©cifiques si n√©cessaire

        domain1_lower = domain1.lower()
        domain2_lower = domain2.lower()

        # R√®gles g√©n√©riques bas√©es sur des patterns linguistiques
        opposite_patterns = [
            (['mechanical', 'physique'], ['optical', 'optique']),
            (['temporal', 'time'], ['spatial', 'space']),
            (['macro', 'large'], ['micro', 'small']),
        ]

        for pattern1, pattern2 in opposite_patterns:
            in_pattern1 = any(p in domain1_lower for p in pattern1)
            in_pattern2 = any(p in domain2_lower for p in pattern2)

            if in_pattern1 and in_pattern2:
                return -0.2  # L√©g√®re incompatibilit√©

        return 0.0  # Neutre

    def _are_hierarchically_related(self, concept1_uri: str, concept2_uri: str) -> bool:
        """V√©rifie si deux concepts sont hi√©rarchiquement li√©s"""
        concept1 = self.ontology_manager.concepts.get(concept1_uri)
        concept2 = self.ontology_manager.concepts.get(concept2_uri)

        if not concept1 or not concept2:
            return False

        # Parent-enfant
        if hasattr(concept1, 'parents'):
            for parent in concept1.parents:
                if hasattr(parent, 'uri') and parent.uri == concept2_uri:
                    return True

        if hasattr(concept2, 'parents'):
            for parent in concept2.parents:
                if hasattr(parent, 'uri') and parent.uri == concept1_uri:
                    return True

        return False

    def _are_semantically_related(self, concept1_uri: str, concept2_uri: str) -> bool:
        """V√©rifie si deux concepts sont s√©mantiquement li√©s"""
        for axiom_type, source, target in self.ontology_manager.axioms:
            if (source == concept1_uri and target == concept2_uri) or \
                    (source == concept2_uri and target == concept1_uri):
                return True
        return False

    def resolve_ambiguities(self, detected_concepts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """R√©sout les ambigu√Øt√©s de mani√®re g√©n√©rique"""

        if not detected_concepts:
            return []

        # Grouper par label
        grouped = {}
        for concept in detected_concepts:
            label = concept.get('label', '').lower().strip()
            if label not in grouped:
                grouped[label] = []
            grouped[label].append(concept)

        resolved = []

        for label, concepts_group in grouped.items():
            if len(concepts_group) == 1:
                # Pas d'ambigu√Øt√© - juste calculer le score contextuel
                concept = concepts_group[0].copy()
                context_score = self._calculate_generic_context_score(concept)
                concept['context_score'] = context_score
                concept['final_confidence'] = concept['confidence'] + (context_score * 0.3)
                resolved.append(concept)

            else:
                # AMBIGU√èT√â - r√©solution g√©n√©rique
                print(f"‚ö†Ô∏è AMBIGU√èT√â sur '{label}' - {len(concepts_group)} variantes")

                best_concept = None
                best_score = -999

                for concept in concepts_group:
                    context_score = self._calculate_generic_context_score(concept)
                    final_score = concept['confidence'] + (context_score * 0.7)

                    print(f"   - {concept.get('concept_uri', '').split('#')[-1]}: "
                          f"base={concept['confidence']:.2f}, "
                          f"context={context_score:.2f}, "
                          f"final={final_score:.2f}")

                    if final_score > best_score:
                        best_score = final_score
                        best_concept = concept.copy()
                        best_concept['context_score'] = context_score
                        best_concept['final_confidence'] = final_score
                        best_concept['ambiguity_resolved'] = True
                        best_concept['resolution_method'] = 'generic_ontological'

                if best_concept:
                    resolved.append(best_concept)
                    chosen = best_concept.get('concept_uri', '').split('#')[-1]
                    print(f"   ‚úÖ CHOIX: {chosen} (score: {best_score:.2f})")

        # Trier par score final
        resolved.sort(key=lambda x: x.get('final_confidence', x['confidence']), reverse=True)
        return resolved

    def _calculate_generic_context_score(self, concept: Dict[str, Any]) -> float:
        """Calcule le score contextuel de mani√®re g√©n√©rique"""

        concept_uri = concept.get('concept_uri', '')
        if not concept_uri:
            return 0.0

        concept_domain = self.concept_domains.get(concept_uri)
        if not concept_domain:
            return 0.0

        print(f"   üéØ Score g√©n√©rique pour {concept.get('label', 'unknown')} (domaine: {concept_domain})")

        total_score = 0.0

        # Pour chaque domaine du contexte document
        for doc_domain, doc_weight in self.document_context.items():
            # R√©cup√©rer la compatibilit√© entre les domaines
            compatibility = self.compatibility_matrix.get((concept_domain, doc_domain), 0.0)

            contribution = doc_weight * compatibility
            total_score += contribution

            print(f"     - vs {doc_domain}: compat={compatibility:.2f}, "
                  f"poids={doc_weight:.2f}, contrib={contribution:.2f}")

        print(f"   ‚úÖ Score final: {total_score:.2f}")
        return total_score


class old_ConceptualPathResolver:
    """R√©sout les ambigu√Øt√©s conceptuelles"""

    def __init__(self, document_concepts: List[Dict[str, Any]]):
        self.document_concepts = document_concepts
        self.document_context = self._extract_document_context()

    def _extract_document_context(self) -> Dict[str, float]:
        """Extrait le contexte s√©mantique du document"""
        context_weights = {}

        print(f"   üß† Analyse du contexte document depuis {len(self.document_concepts)} concepts")

        for concept in self.document_concepts:
            label = concept.get('label', '').lower()
            confidence = concept.get('confidence', 0)

            # Patterns de classification
            if any(keyword in label for keyword in
                   ['m√©canique', 'mechanical', 'precision', 'dimension', 'tolerance', 'distance']):
                context_weights['mechanical'] = context_weights.get('mechanical', 0) + confidence
                print(f"     - {label} ‚Üí mechanical (+{confidence:.2f})")

            if any(keyword in label for keyword in
                   ['optique', 'optical', 'longueur d\'onde', 'wavelength', 'spectro', 'lumi√®re', 'light']):
                context_weights['optical'] = context_weights.get('optical', 0) + confidence
                print(f"     - {label} ‚Üí optical (+{confidence:.2f})")

        # Normaliser
        total_weight = sum(context_weights.values())
        if total_weight > 0:
            context_weights = {k: v / total_weight for k, v in context_weights.items()}

        print(f"   ‚úÖ Contexte document: {context_weights}")
        return context_weights

    def resolve_ambiguities(self, detected_concepts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """R√©sout les ambigu√Øt√©s selon le contexte du document"""

        if not detected_concepts:
            return []

        # Grouper par label pour identifier les ambigu√Øt√©s
        grouped = {}
        for concept in detected_concepts:
            label = concept.get('label', '').lower()
            if label not in grouped:
                grouped[label] = []
            grouped[label].append(concept)

        resolved = []

        for label, concepts_group in grouped.items():
            if len(concepts_group) == 1:
                # Pas d'ambigu√Øt√©
                resolved.append(concepts_group[0])
            else:
                # Ambigu√Øt√© d√©tect√©e
                print(f"   ‚ö†Ô∏è  AMBIGU√èT√â sur '{label}' - {len(concepts_group)} variantes")

                best_concept = None
                best_score = -999

                for concept in concepts_group:
                    score = self._calculate_context_score(concept)
                    print(f"     - {concept.get('concept_uri', '').split('#')[-1]}: score {score:.2f}")

                    if score > best_score:
                        best_score = score
                        best_concept = concept.copy()
                        best_concept['ambiguity_resolved'] = True
                        best_concept['context_score'] = score

                if best_concept:
                    chosen_variant = best_concept.get('concept_uri', '').split('#')[-1]
                    print(f"   ‚úÖ Choix: {chosen_variant} (score: {best_score:.2f})")
                    resolved.append(best_concept)

        return resolved

    def _calculate_context_score(self, concept: Dict[str, Any]) -> float:
        """Calcule le score contextuel d'un concept"""
        label = concept.get('label', '').lower()
        uri = concept.get('concept_uri', '').lower()
        base_confidence = concept.get('confidence', 0)

        context_bonus = 0.0

        for context_type, weight in self.document_context.items():
            if context_type == 'mechanical':
                if any(keyword in label or keyword in uri for keyword in ['distance', 'mechanical', 'precision']):
                    context_bonus += weight * 0.5
                elif any(keyword in label or keyword in uri for keyword in ['wavelength', 'optical', 'light']):
                    context_bonus -= weight * 0.5

            elif context_type == 'optical':
                if any(keyword in label or keyword in uri for keyword in ['wavelength', 'optical', 'light']):
                    context_bonus += weight * 0.5
                elif any(keyword in label or keyword in uri for keyword in ['distance', 'mechanical', 'precision']):
                    context_bonus -= weight * 0.5

        return base_confidence + context_bonus


class HierarchicalSemanticChunker:
    """Chunker s√©mantique avec navigation ontologique par chemin"""

    def __init__(self, min_chunk_size: int = 200, max_chunk_size: int = 1000,
                 overlap_sentences: int = 2, ontology_manager=None, concept_classifier=None):
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.overlap_sentences = overlap_sentences
        self.ontology_manager = ontology_manager
        self.concept_classifier = concept_classifier

        # Navigateur ontologique
        if ontology_manager and concept_classifier:
            self.path_navigator = OntologyPathNavigator(ontology_manager, concept_classifier)
        else:
            self.path_navigator = None


        self.selective_detector = None

        # R√©solveurs de chemin par document
        self.document_path_resolvers = {}  # document_id -> ConceptualPathResolver
        self.document_resolvers = {}

    async def _process_section_selective(self, section: Dict[str, Any],
                                         document_concepts: List[Dict[str, Any]],
                                         document_id: str, filepath: str,
                                         metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Traite une section avec d√©tection s√©lective"""

        section_title = section.get('title', 'Sans titre')
        section_text = section.get('text', '')

        print(f"üîç Section: {section_title}")

        if not section_text.strip():
            return []

        # D√©tection cibl√©e au niveau section
        section_concepts = await self.selective_detector.detect_section_level_concepts(
            section_text, document_concepts, max_concepts=8
        )

        # R√©soudre les ambigu√Øt√©s avec le contexte document
        resolver = self.document_resolvers.get(document_id)
        if resolver:
            section_concepts = resolver.resolve_ambiguities(section_concepts)

        if section_concepts:
            print(f"   üìã {len(section_concepts)} concepts retenus:")
            for concept in section_concepts[:3]:
                distance = concept.get('path_distance', 0)
                resolved = " (r√©solu)" if concept.get('ambiguity_resolved') else ""
                print(f"      - {concept['label']} (dist: {distance}, conf: {concept['confidence']:.2f}){resolved}")

        # Cr√©er le chunk
        chunk = SemanticChunk(
            chunk_id=f"{document_id}-section-{section.get('section_id', 'unknown')}",
            text=section_text,
            start_pos=section.get('start_line', 0),
            end_pos=section.get('end_line', 0),
            level='section',
            concepts=section_concepts,
            inherited_concepts=document_concepts,
            metadata={
                'section_title': section_title,
                'section_level': section.get('level', 0),
                'selective_detection': True,
                'concepts_count': len(section_concepts),
                'inherited_count': len(document_concepts)
            }
        )

        chunk_dict = await self._chunk_to_dict(chunk, document_id, filepath, metadata)
        return [chunk_dict]

    async def create_semantic_chunks(self, text: str, document_id: str, filepath: str,
                                     metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Chunking avec d√©tection s√©lective"""

        filename = Path(filepath).name
        print(f"üìÑ Chunking s√©lectif: {filename}")

        if not self.selective_detector:
            print(f"   ‚ö†Ô∏è  Pas de d√©tecteur s√©lectif - fallback")
            return await self._fallback_chunking(text, document_id, filepath, metadata)

        try:
            # √âTAPE 1: D√©tection S√âLECTIVE au niveau document
            document_concepts = await self.selective_detector.detect_document_level_concepts(text, max_concepts=5)

            if not document_concepts:
                print(f"   ‚ö†Ô∏è  Aucun concept dominant d√©tect√©")
                return await self._fallback_chunking(text, document_id, filepath, metadata)

            # √âTAPE 2: Cr√©er le r√©solveur contextuel
            resolver = ConceptualPathResolver(document_concepts, self.ontology_manager)
            self.document_resolvers[document_id] = resolver

            # √âTAPE 3: Traiter les sections avec d√©tection cibl√©e
            sections = self._extract_sections(text)
            all_chunks = []

            for section in sections:
                section_chunks = await self._process_section_selective(
                    section, document_concepts, document_id, filepath, metadata
                )
                all_chunks.extend(section_chunks)

            # √âTAPE 4: Optimiser
            optimized_chunks = self._optimize_chunks(all_chunks)

            print(f"‚úÖ Chunking s√©lectif termin√©: {len(optimized_chunks)} chunks")
            return optimized_chunks

        except Exception as e:
            print(f"‚ùå Erreur chunking s√©lectif: {e}")
            return await self._fallback_chunking(text, document_id, filepath, metadata)

    def set_ontology_components(self, ontology_manager, concept_classifier):
        """Configure les composants ontologiques apr√®s l'initialisation"""
        self.ontology_manager = ontology_manager
        self.concept_classifier = concept_classifier

        # Recr√©er le navigateur ontologique
        if ontology_manager and concept_classifier:
            self.path_navigator = OntologyPathNavigator(ontology_manager, concept_classifier)
            self.selective_detector = SelectiveConceptDetector(concept_classifier, ontology_manager)
            print("‚úÖ Navigateur ontologique configur√©")
        else:
            self.path_navigator = None
            print("‚ö†Ô∏è Composants ontologiques manquants")

    async def old_create_semantic_chunks(self, text: str, document_id: str, filepath: str,
                                     metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Cr√©e des chunks avec navigation ontologique"""

        filename = Path(filepath).name
        print(f"üìÑ Chunking avec navigation ontologique: {filename}")

        try:
            # √âTAPE 1: D√©tection des concepts au niveau document (point de d√©part)
            document_concepts = await self._detect_document_concepts(text, document_id, filename)

            if not document_concepts:
                print(f"   ‚ö†Ô∏è  Aucun concept d√©tect√© - fallback")
                return await self._fallback_chunking(text, document_id, filepath, metadata)

            # √âTAPE 2: Cr√©er le r√©solveur de chemin pour ce document
            path_resolver = ConceptualPathResolver(document_concepts)
            self.document_path_resolvers[document_id] = path_resolver

            # √âTAPE 3: Diviser en sections
            sections = self._extract_sections(text)

            # √âTAPE 4: Traiter chaque section avec navigation ontologique
            all_chunks = []

            for section in sections:
                section_chunks = await self._process_section_with_path(
                    section, document_concepts, document_id, filepath, metadata
                )
                all_chunks.extend(section_chunks)

            # √âTAPE 5: Optimiser et finaliser
            optimized_chunks = self._optimize_chunks(all_chunks)

            print(f"‚úÖ Chunking termin√©: {len(optimized_chunks)} chunks cr√©√©s")
            return optimized_chunks

        except Exception as e:
            print(f"‚ùå Erreur chunking: {e}")
            import traceback
            traceback.print_exc()
            return await self._fallback_chunking(text, document_id, filepath, metadata)

    async def _detect_document_concepts(self, text: str, document_id: str, filename: str) -> List[Dict[str, Any]]:
        """D√©tecte les concepts au niveau document (point de d√©part)"""

        print(f"üß† D√©tection concepts document: {filename}")

        if not self.path_navigator:
            print(f"   ‚ö†Ô∏è  Pas de navigateur ontologique")
            return []

        # D√©tection libre au niveau document
        concepts = await self.path_navigator.detect_concepts_in_path(text, [])

        if concepts:
            print(f"   üìã {len(concepts)} concepts d√©tect√©s (point de d√©part)")
            for concept in concepts[:5]:
                print(f"      - {concept['label']} (conf: {concept['confidence']:.2f})")
        else:
            print(f"   ‚ö†Ô∏è  Aucun concept d√©tect√© au niveau document")

        return concepts

    async def _process_section_with_path(self, section: Dict[str, Any],
                                         document_concepts: List[Dict[str, Any]],
                                         document_id: str, filepath: str,
                                         metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Traite une section avec navigation ontologique"""

        section_title = section.get('title', 'Sans titre')
        section_text = section.get('text', '')

        print(f"üîç Section: {section_title}")

        if not section_text.strip():
            return []

        # NAVIGATION ONTOLOGIQUE: D√©tecter les concepts dans le chemin
        section_concepts = await self.path_navigator.detect_concepts_in_path(
            section_text, document_concepts  # ‚Üê H√©ritage depuis le document
        )

        # R√©soudre les ambigu√Øt√©s avec le contexte du document
        resolver = self.document_path_resolvers.get(document_id)
        if resolver:
            section_concepts = resolver.resolve_ambiguities(section_concepts)

        # Cr√©er le chunk de section
        if section_concepts:
            print(f"   üìã {len(section_concepts)} concepts retenus:")
            for concept in section_concepts[:3]:
                in_path = " (chemin)" if concept.get('in_ontology_path') else ""
                resolved = " (r√©solu)" if concept.get('ambiguity_resolved') else ""
                print(f"      - {concept['label']} (conf: {concept['confidence']:.2f}){in_path}{resolved}")

        # Cr√©er le chunk s√©mantique
        chunk = SemanticChunk(
            chunk_id=f"{document_id}-section-{section.get('section_id', 'unknown')}",
            text=section_text,
            start_pos=section.get('start_line', 0),
            end_pos=section.get('end_line', 0),
            level='section',
            concepts=section_concepts,
            inherited_concepts=document_concepts,
            metadata={
                'section_title': section_title,
                'section_level': section.get('level', 0),
                'ontology_path_applied': True,
                'concepts_count': len(section_concepts),
                'inherited_count': len(document_concepts)
            }
        )

        # Convertir en format standard
        chunk_dict = await self._chunk_to_dict(chunk, document_id, filepath, metadata)
        return [chunk_dict]

    def _extract_sections(self, text: str) -> List[Dict[str, Any]]:
        """Extrait les sections du texte"""
        sections = []
        lines = text.split('\n')
        current_section = {'lines': [], 'title': '', 'level': 0, 'start_line': 0}
        line_number = 0

        for line in lines:
            line_number += 1

            # D√©tecter les headers
            header_info = self._detect_header(line)

            if header_info:
                # Finaliser section pr√©c√©dente
                if current_section['lines']:
                    current_section['end_line'] = line_number - 1
                    current_section['text'] = '\n'.join(current_section['lines'])
                    current_section['section_id'] = f"section_{len(sections)}"
                    sections.append(current_section)

                # Nouvelle section
                level, title, header_type = header_info
                current_section = {
                    'lines': [line],
                    'title': title,
                    'level': level,
                    'start_line': line_number,
                    'header_type': header_type
                }
            else:
                current_section['lines'].append(line)

        # Derni√®re section
        if current_section['lines']:
            current_section['end_line'] = line_number
            current_section['text'] = '\n'.join(current_section['lines'])
            current_section['section_id'] = f"section_{len(sections)}"
            sections.append(current_section)

        return sections

    def _detect_header(self, line: str) -> Optional[Tuple[int, str, str]]:
        """D√©tecte les headers"""
        line = line.strip()

        # Markdown headers
        if line.startswith('#'):
            level = len(line) - len(line.lstrip('#'))
            title = line.lstrip('#').strip()
            if title:
                return (level, title, 'markdown')

        # Headers num√©rot√©s
        match = re.match(r'^((?:\d+\.)+)\s*(.+)$', line)
        if match:
            level = match.group(1).count('.')
            title = match.group(2).strip()
            return (level, title, 'numbered')

        # Headers majuscules
        if line.isupper() and len(line) > 3 and not line.endswith('.'):
            return (1, line, 'capital')

        return None

    async def _chunk_to_dict(self, chunk: SemanticChunk, document_id: str,
                             filepath: str, base_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Convertit un SemanticChunk en dictionnaire"""

        content_hash = hashlib.md5(chunk.text.encode()).hexdigest()

        metadata = {
            **base_metadata,
            **chunk.metadata,
            'detected_concepts': chunk.concepts,
            'inherited_concepts': chunk.inherited_concepts,

            # CRITIQUE : chunk_level doit √™tre dans les m√©tadonn√©es persistantes
            'chunk_level': chunk.level,  # ‚Üê IMPORTANT !

            'content_hash': content_hash,
            'chunk_method': 'ontology_path_navigation',
            'created_at': datetime.now().isoformat(),
            'filepath': filepath,
            'filename': os.path.basename(filepath),
            'start_pos': chunk.start_pos,
            'end_pos': chunk.end_pos,
            'section_title': chunk.metadata.get('section_title', 'Section sans titre'),
            'section_level': chunk.metadata.get('section_level', 0)
        }

        return {
            "id": chunk.chunk_id,
            "document_id": document_id,
            "text": chunk.text,
            "start_pos": chunk.start_pos,
            "end_pos": chunk.end_pos,
            "metadata": metadata
        }

    def _optimize_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Optimise la collection de chunks"""
        if not chunks:
            return chunks

        # √âliminer doublons
        seen_hashes = set()
        unique_chunks = []

        for chunk in chunks:
            content_hash = chunk['metadata']['content_hash']
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                unique_chunks.append(chunk)

        # Trier par position
        unique_chunks.sort(key=lambda x: x.get('start_pos', 0))

        # Renum√©roter
        for i, chunk in enumerate(unique_chunks):
            chunk['id'] = f"{chunk['document_id']}-chunk-{i}"
            chunk['metadata']['chunk_index'] = i
            chunk['metadata']['total_chunks'] = len(unique_chunks)

        return unique_chunks

    async def _fallback_chunking(self, text: str, document_id: str, filepath: str,
                                 metadata: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Chunking de fallback"""
        print(f"üîÑ Chunking de fallback")

        sentences = self._split_into_sentences(text)
        chunks = []

        current_sentences = []
        current_size = 0

        for sentence in sentences:
            sentence_size = len(sentence)

            if current_size + sentence_size > self.max_chunk_size and current_sentences:
                chunk_text = ' '.join(current_sentences)
                chunk = {
                    "id": f"{document_id}-fallback-{len(chunks)}",
                    "document_id": document_id,
                    "text": chunk_text,
                    "start_pos": text.find(current_sentences[0]),
                    "end_pos": text.find(current_sentences[-1]) + len(current_sentences[-1]),
                    "metadata": {
                        "filepath": filepath,
                        "filename": os.path.basename(filepath),
                        "chunk_method": "fallback",
                        "detected_concepts": [],
                        **(metadata or {})
                    }
                }
                chunks.append(chunk)
                current_sentences = []
                current_size = 0

            current_sentences.append(sentence)
            current_size += sentence_size + 1

        if current_sentences:
            chunk_text = ' '.join(current_sentences)
            chunk = {
                "id": f"{document_id}-fallback-{len(chunks)}",
                "document_id": document_id,
                "text": chunk_text,
                "start_pos": text.find(current_sentences[0]),
                "end_pos": text.find(current_sentences[-1]) + len(current_sentences[-1]),
                "metadata": {
                    "filepath": filepath,
                    "filename": os.path.basename(filepath),
                    "chunk_method": "fallback",
                    "detected_concepts": [],
                    **(metadata or {})
                }
            }
            chunks.append(chunk)

        return chunks

    def _split_into_sentences(self, text: str) -> List[str]:
        """Divise en phrases"""
        pattern = r'(?<=[.!?])\s+(?=[A-Z])'
        sentences = re.split(pattern, text)
        return [s.strip() for s in sentences if s.strip()]


# Wrapper pour compatibilit√©
class SemanticChunker:
    """Wrapper pour compatibilit√©"""

    def __init__(self, min_chunk_size: int = 200, max_chunk_size: int = 1000, overlap_sentences: int = 2):
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.overlap_sentences = overlap_sentences
        self.hierarchical_chunker = None

    def set_ontology_manager(self, ontology_manager, concept_classifier):
        """Configure les composants ontologiques"""
        self.hierarchical_chunker = HierarchicalSemanticChunker(
            min_chunk_size=self.min_chunk_size,
            max_chunk_size=self.max_chunk_size,
            overlap_sentences=self.overlap_sentences,
            ontology_manager=ontology_manager,
            concept_classifier=concept_classifier
        )

    def create_semantic_chunks(self, text: str, document_id: str, filepath: str,
                               metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Version synchrone"""
        if self.hierarchical_chunker:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(
                    self.hierarchical_chunker.create_semantic_chunks(text, document_id, filepath, metadata)
                )
            finally:
                loop.close()
        else:
            return self._simple_chunking(text, document_id, filepath, metadata)

    def _simple_chunking(self, text: str, document_id: str, filepath: str,
                         metadata: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Chunking simple"""
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)
        chunks = []
        current_chunk = []
        current_size = 0

        for sentence in sentences:
            if current_size + len(sentence) > self.max_chunk_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                chunks.append({
                    "id": f"{document_id}-simple-{len(chunks)}",
                    "document_id": document_id,
                    "text": chunk_text,
                    "start_pos": text.find(current_chunk[0]),
                    "end_pos": text.find(current_chunk[-1]) + len(current_chunk[-1]),
                    "metadata": {
                        "filepath": filepath,
                        "filename": os.path.basename(filepath),
                        "chunk_method": "simple",
                        "detected_concepts": [],
                        **(metadata or {})
                    }
                })
                current_chunk = []
                current_size = 0

            current_chunk.append(sentence)
            current_size += len(sentence) + 1

        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            chunks.append({
                "id": f"{document_id}-simple-{len(chunks)}",
                "document_id": document_id,
                "text": chunk_text,
                "start_pos": text.find(current_chunk[0]),
                "end_pos": text.find(current_chunk[-1]) + len(current_chunk[-1]),
                "metadata": {
                    "filepath": filepath,
                    "filename": os.path.basename(filepath),
                    "chunk_method": "simple",
                    "detected_concepts": [],
                    **(metadata or {})
                }
            })

        return chunks


class ConceptualHierarchicalEngine:
    """Moteur de recherche hi√©rarchique unifi√© pour texte ET Fortran"""

    def __init__(self, document_store, embedding_provider, entity_manager=None):
        self.document_store = document_store
        self.embedding_provider = embedding_provider
        self.entity_manager = entity_manager

        # Moteur texte existant
        self.text_engine = LevelBasedSearchEngine(document_store, embedding_provider)

    async def intelligent_hierarchical_search(
            self,
            query: str,
            max_per_level: int = 3,
            mode: str = 'auto'  # 'auto', 'text', 'fortran', 'unified'
    ) -> Dict[str, Any]:
        """Recherche hi√©rarchique intelligente avec auto-d√©tection"""

        print(f"üîç Recherche hi√©rarchique intelligente: {query} (mode: {mode})")

        # 1. Analyser le contenu disponible
        content_analysis = await self._analyze_available_content()

        # 2. D√©terminer la strat√©gie de recherche
        if mode == 'auto':
            search_mode = await self._detect_optimal_mode(query, content_analysis)
        else:
            search_mode = mode

        print(f"   üéØ Mode d√©tect√©/choisi: {search_mode}")

        # 3. Recherche selon le mode
        if search_mode == 'fortran':
            return await self._fortran_hierarchical_search(query, max_per_level)
        elif search_mode == 'text':
            return await self._text_hierarchical_search(query, max_per_level)
        elif search_mode == 'unified':
            return await self._unified_hierarchical_search(query, max_per_level)
        else:
            # Fallback
            return await self._unified_hierarchical_search(query, max_per_level)

    async def _analyze_available_content(self) -> Dict[str, Any]:
        """Analyse le contenu disponible pour d√©terminer les types"""

        analysis = {
            'total_chunks': 0,
            'fortran_chunks': 0,
            'text_chunks': 0,
            'conceptual_chunks': 0,
            'content_types': set()
        }

        # Parcourir tous les documents
        for doc_id, chunks in self.document_store.document_chunks.items():
            for chunk in chunks:
                analysis['total_chunks'] += 1

                metadata = chunk.get('metadata', {})
                content_type = metadata.get('content_type', 'unknown')
                analysis['content_types'].add(content_type)

                if content_type == 'fortran':
                    analysis['fortran_chunks'] += 1
                elif metadata.get('chunk_level') in ['document', 'section', 'paragraph']:
                    analysis['text_chunks'] += 1

                if metadata.get('conceptual_level'):
                    analysis['conceptual_chunks'] += 1

        print(f"   üìä Analyse contenu: {analysis['fortran_chunks']} Fortran, "
              f"{analysis['text_chunks']} texte, {analysis['conceptual_chunks']} conceptuel")

        return analysis

    async def _detect_optimal_mode(self, query: str, content_analysis: Dict[str, Any]) -> str:
        """D√©tecte le mode optimal selon la requ√™te et le contenu"""

        query_lower = query.lower()

        # Patterns Fortran
        fortran_indicators = [
            'function', 'subroutine', 'module', 'program',
            'call', 'use', 'interface', 'type',
            '.f90', '.f95', 'fortran'
        ]

        # Patterns texte
        text_indicators = [
            'section', 'paragraphe', 'chapitre', 'document',
            'r√©sum√©', 'introduction', 'conclusion'
        ]

        # Score de la requ√™te
        fortran_score = sum(1 for indicator in fortran_indicators if indicator in query_lower)
        text_score = sum(1 for indicator in text_indicators if indicator in query_lower)

        # Score du contenu disponible
        fortran_ratio = content_analysis['fortran_chunks'] / max(1, content_analysis['total_chunks'])
        text_ratio = content_analysis['text_chunks'] / max(1, content_analysis['total_chunks'])

        # D√©cision
        if fortran_score > text_score and fortran_ratio > 0.3:
            return 'fortran'
        elif text_score > fortran_score and text_ratio > 0.3:
            return 'text'
        elif content_analysis['conceptual_chunks'] > content_analysis['total_chunks'] * 0.5:
            return 'unified'  # La majorit√© du contenu est conceptuellement enrichi
        else:
            return 'unified'  # Mode par d√©faut

    async def _fortran_hierarchical_search(self, query: str, max_per_level: int) -> Dict[str, Any]:
        """Recherche hi√©rarchique sp√©cialis√©e Fortran"""

        print(f"   üîß Recherche Fortran hi√©rarchique")

        # Niveaux Fortran
        fortran_levels = {
            'container': ['module', 'program'],
            'component': ['function', 'subroutine', 'type_definition', 'interface'],
            'detail': ['internal_function', 'variable_declaration', 'parameter']
        }

        results = {}

        for conceptual_level, native_types in fortran_levels.items():
            level_results = await self._search_fortran_level(query, native_types, max_per_level)
            if level_results:
                results[conceptual_level] = {
                    'results': level_results,
                    'native_types': native_types,
                    'display_name': self._get_fortran_level_display_name(conceptual_level)
                }

        return {
            'search_mode': 'fortran',
            'hierarchical_results': results,
            'total_levels': len(results)
        }

    async def _text_hierarchical_search(self, query: str, max_per_level: int) -> Dict[str, Any]:
        """Recherche hi√©rarchique texte (existante)"""

        print(f"   üìÑ Recherche texte hi√©rarchique")

        # Utiliser le moteur existant
        text_results = await self.text_engine.hierarchical_search(query, max_per_level)

        # Reformater pour uniformit√©
        unified_results = {}
        level_mapping = {
            'document': 'container',
            'section': 'component',
            'paragraph': 'detail'
        }

        for text_level, results in text_results.items():
            conceptual_level = level_mapping.get(text_level, text_level)
            unified_results[conceptual_level] = {
                'results': results,
                'native_types': [text_level],
                'display_name': self._get_text_level_display_name(text_level)
            }

        return {
            'search_mode': 'text',
            'hierarchical_results': unified_results,
            'total_levels': len(unified_results)
        }

    async def _unified_hierarchical_search(self, query: str, max_per_level: int) -> Dict[str, Any]:
        """Recherche hi√©rarchique unifi√©e (texte + Fortran)"""

        print(f"   üåê Recherche unifi√©e hi√©rarchique")

        conceptual_levels = ['container', 'component', 'detail']
        results = {}

        for conceptual_level in conceptual_levels:
            level_results = await self._search_unified_conceptual_level(
                query, conceptual_level, max_per_level
            )
            if level_results:
                results[conceptual_level] = {
                    'results': level_results,
                    'display_name': self._get_unified_level_display_name(conceptual_level)
                }

        return {
            'search_mode': 'unified',
            'hierarchical_results': results,
            'total_levels': len(results)
        }

    async def _search_fortran_level(self, query: str, native_types: List[str], max_results: int) -> List[
        Dict[str, Any]]:
        """Recherche dans un niveau Fortran sp√©cifique"""

        if not self.entity_manager:
            return []

        all_results = []

        # Rechercher par type d'entit√©
        for entity_type in native_types:
            entities = await self.entity_manager.get_entities_by_type(entity_type)

            for entity in entities:
                # Score de pertinence simple
                score = self._calculate_entity_relevance(query, entity)
                if score > 0.3:  # Seuil de pertinence
                    all_results.append({
                        'entity': entity,
                        'similarity': score,
                        'source_info': {
                            'filename': entity.filename,
                            'start_line': entity.start_line,
                            'end_line': entity.end_line,
                            'entity_type': entity.entity_type,
                            'entity_name': entity.entity_name
                        }
                    })

        # Trier par pertinence
        all_results.sort(key=lambda x: x['similarity'], reverse=True)
        return all_results[:max_results]

    async def _search_unified_conceptual_level(self, query: str, conceptual_level: str, max_results: int) -> List[
        Dict[str, Any]]:
        """Recherche dans un niveau conceptuel unifi√©"""

        # G√©n√©rer l'embedding de la requ√™te
        query_embedding = await self.embedding_provider.generate_embeddings([query])
        query_embedding = query_embedding[0]

        level_chunks = []

        # Collecter les chunks du niveau conceptuel
        for doc_id, chunks in self.document_store.document_chunks.items():
            for chunk in chunks:
                metadata = chunk.get('metadata', {})
                chunk_conceptual_level = metadata.get('conceptual_level')

                # Fallback vers le mapping texte traditionnel
                if not chunk_conceptual_level:
                    text_level = metadata.get('chunk_level')
                    text_to_conceptual = {
                        'document': 'container',
                        'section': 'component',
                        'paragraph': 'detail'
                    }
                    chunk_conceptual_level = text_to_conceptual.get(text_level)

                if chunk_conceptual_level == conceptual_level:
                    level_chunks.append(chunk)

        if not level_chunks:
            return []

        # Calculer les similarit√©s
        similarities = []
        for chunk in level_chunks:
            chunk_id = chunk['id']
            chunk_embedding = self.document_store.embedding_manager.get_embedding(chunk_id)

            if chunk_embedding is not None:
                similarity = float(np.dot(query_embedding, chunk_embedding))

                metadata = chunk.get('metadata', {})
                similarities.append({
                    'chunk': chunk,
                    'similarity': similarity,
                    'source_info': {
                        'filename': metadata.get('filename', 'Unknown'),
                        'start_line': metadata.get('start_pos', 0),
                        'end_line': metadata.get('end_pos', 0),
                        'section_title': metadata.get('section_title') or metadata.get('entity_name', 'Sans titre'),
                        'content_type': metadata.get('content_type', 'unknown'),
                        'native_type': metadata.get('native_type') or metadata.get('chunk_level', 'unknown')
                    }
                })

        # Trier et limiter
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        return similarities[:max_results]

    def _calculate_entity_relevance(self, query: str, entity) -> float:
        """Calcule la pertinence d'une entit√© Fortran pour une requ√™te"""
        query_lower = query.lower()
        entity_name_lower = entity.entity_name.lower()

        # Score de base sur le nom
        if query_lower == entity_name_lower:
            return 1.0
        elif query_lower in entity_name_lower:
            return 0.8
        elif entity_name_lower in query_lower:
            return 0.7

        # Score sur les concepts
        concept_score = 0.0
        for concept_label in entity.concepts:
            if query_lower in concept_label.lower():
                concept_score = max(concept_score, 0.6)

        # Score sur les d√©pendances
        dep_score = 0.0
        for dep in entity.dependencies:
            if query_lower in dep.lower():
                dep_score = max(dep_score, 0.5)

        return max(concept_score, dep_score)

    def _get_fortran_level_display_name(self, conceptual_level: str) -> str:
        """Nom d'affichage pour les niveaux Fortran"""
        names = {
            'container': 'Modules/Programmes',
            'component': 'Fonctions/Subroutines',
            'detail': 'D√©tails/Variables'
        }
        return names.get(conceptual_level, conceptual_level)

    def _get_text_level_display_name(self, text_level: str) -> str:
        """Nom d'affichage pour les niveaux texte"""
        names = {
            'document': 'Documents',
            'section': 'Sections',
            'paragraph': 'Paragraphes'
        }
        return names.get(text_level, text_level)

    def _get_unified_level_display_name(self, conceptual_level: str) -> str:
        """Nom d'affichage pour les niveaux unifi√©s"""
        names = {
            'container': 'Conteneurs (Modules/Documents)',
            'component': 'Composants (Fonctions/Sections)',
            'detail': 'D√©tails (Variables/Paragraphes)'
        }
        return names.get(conceptual_level, conceptual_level)