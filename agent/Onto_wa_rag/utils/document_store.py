"""
    ------------------------------------------
    Copyright: CEA Grenoble
    Auteur: Yoann CURE
    Entit√©: IRIG
    Ann√©e: 2025
    Description: Agent IA d'Int√©gration Continue
    ------------------------------------------
    """

# document_store.py
import asyncio
import hashlib
import json
import os
import shutil
from pathlib import Path
from typing import List, Dict, Any, Optional
import pickle

import aiohttp


class DocumentStore:
    """Stockage des documents et gestion de leurs m√©tadonn√©es"""

    def __init__(
            self,
            document_processor,
            embedding_manager,
            storage_dir: str = "storage"
    ):
        """
        Initialise le stockage de documents

        Args:
            document_processor: Processeur pour traiter les documents
            embedding_manager: Gestionnaire d'embeddings
            storage_dir: R√©pertoire pour stocker les documents et m√©tadonn√©es
        """
        self.processor = document_processor
        self.embedding_manager = embedding_manager
        self.storage_dir = storage_dir
        self.documents_dir = os.path.join(storage_dir, "documents")
        self.metadata_path = os.path.join(storage_dir, "metadata.json")

        # Dictionnaire pour stocker les m√©tadonn√©es des documents
        self.documents: Dict[str, Dict[str, Any]] = {}

        # Dictionnaire pour stocker les chunks de documents
        self.document_chunks: Dict[str, List[Dict[str, Any]]] = {}

        # Cr√©er les r√©pertoires n√©cessaires
        os.makedirs(self.storage_dir, exist_ok=True)
        os.makedirs(self.documents_dir, exist_ok=True)

        # Charger les m√©tadonn√©es existantes
        self._load_metadata()

    async def add_document_with_id(self,
                                   filepath: str,
                                   document_id: str,
                                   additional_metadata: Optional[Dict[str, Any]] = None
                                   ) -> str:
        """
        Ajoute un document avec un ID sp√©cifique en assurant la coh√©rence avec metadata.json
        """
        # V√©rifier si le document existe d√©j√†
        if document_id in self.documents:
            doc_info = self.documents[document_id]
            doc_path = doc_info.get("path", "")
            if os.path.exists(doc_path):
                print(f"Document {document_id} d√©j√† pr√©sent avec fichier existant.")
                chunks_path = os.path.join(self.storage_dir, "chunks", f"{document_id}.pkl")
                embedding_path = os.path.join(self.embedding_manager.storage_dir, f"{document_id}.pkl")
                if os.path.exists(chunks_path) and os.path.exists(embedding_path):
                    return document_id
                print(f"Fichiers associ√©s manquants pour document {document_id}, r√©indexation...")

        # Traiter le document
        _, chunks = await self.processor.process_document(filepath, document_id, additional_metadata)

        # üîë G√©n√©rer un nom de fichier court et stable
        if filepath.startswith(("http://", "https://")):
            original_filename = Path(filepath).name or "remote_file"
        else:
            original_filename = os.path.basename(filepath)

        # Hash pour √©viter les noms trop longs ou caract√®res interdits
        short_hash = hashlib.md5(filepath.encode("utf-8")).hexdigest()[:8]
        safe_filename = f"{document_id}_{short_hash}.txt"

        document_path = os.path.join(self.documents_dir, safe_filename)

        loop = asyncio.get_event_loop()
        if filepath.startswith(("http://", "https://")):
            async with aiohttp.ClientSession() as session:
                async with session.get(filepath) as response:
                    response.raise_for_status()
                    content = await response.read()
                    await loop.run_in_executor(None, lambda: open(document_path, "wb").write(content))
        else:
            await loop.run_in_executor(None, lambda: shutil.copy2(filepath, document_path))

        # Stocker m√©tadonn√©es
        self.documents[document_id] = {
            "id": document_id,
            "path": document_path,
            "original_path": filepath,
            "original_filename": original_filename,
            "chunks_count": len(chunks),
            "beir_id": document_id,
            "additional_metadata": additional_metadata
        }

        # Stockage des chunks
        self.document_chunks[document_id] = chunks
        chunks_dir = os.path.join(self.storage_dir, "chunks")
        os.makedirs(chunks_dir, exist_ok=True)
        chunks_path = os.path.join(chunks_dir, f"{document_id}.pkl")
        await loop.run_in_executor(None, lambda: self._save_chunks_sync(chunks_path, chunks))

        # Embeddings
        embedding_path = os.path.join(self.embedding_manager.storage_dir, f"{document_id}.pkl")
        if os.path.exists(embedding_path):
            print(f"Embeddings existants trouv√©s pour {document_id}, chargement...")
            await self.embedding_manager.load_embeddings(document_id)
        else:
            print(f"Cr√©ation des embeddings pour {document_id}...")
            await self.embedding_manager.create_embeddings(chunks)
            await self.embedding_manager.save_embeddings(document_id)

        await self._save_metadata()
        return document_id

    def _load_metadata(self) -> None:
        """Charge les m√©tadonn√©es des documents depuis le fichier et v√©rifie leur coh√©rence"""
        if os.path.exists(self.metadata_path):
            try:
                with open(self.metadata_path, 'r', encoding='utf-8') as f:
                    self.documents = json.load(f)

                # V√©rifier la coh√©rence des entr√©es dans metadata.json
                inconsistent_docs = []
                for doc_id, doc_info in self.documents.items():
                    # V√©rifier si le fichier du document existe
                    doc_path = doc_info.get("path", "")
                    if not os.path.exists(doc_path):
                        print(f"‚ö†Ô∏è Fichier manquant pour document {doc_id}: {doc_path}")
                        inconsistent_docs.append(doc_id)

                # Option: supprimer les entr√©es incoh√©rentes
                for doc_id in inconsistent_docs:
                    del self.documents[doc_id]

                print(f"M√©tadonn√©es charg√©es: {len(self.documents)} documents")
                if inconsistent_docs:
                    print(f"Attention: {len(inconsistent_docs)} documents ont des incoh√©rences")

            except Exception as e:
                print(f"Erreur lors du chargement des m√©tadonn√©es: {str(e)}")
                self.documents = {}
        else:
            print(f"Fichier de m√©tadonn√©es non trouv√©: {self.metadata_path}")
            self.documents = {}

    async def old_save_metadata(self):
        """Sauvegarde les m√©tadonn√©es des documents"""
        try:
            metadata = {
                "documents": self.documents,
                "document_chunks": {}
            }

            # CORRECTION : Cr√©er une copie de la liste des cl√©s pour √©viter l'erreur
            document_ids = list(self.document_chunks.keys())
            for document_id in document_ids:
                if document_id in self.document_chunks:  # V√©rifier que la cl√© existe encore
                    metadata["document_chunks"][document_id] = len(self.document_chunks[document_id])

            metadata_path = self.storage_dir / "metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2)

        except Exception as e:
            print(f"Erreur lors de la sauvegarde des m√©tadonn√©es: {e}")

    async def _save_metadata(self) -> None:
        """Sauvegarde les m√©tadonn√©es des documents dans le fichier"""
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            None,
            lambda: self._save_metadata_sync()
        )

        # Sauvegarder √©galement les chunks pour chaque document
        for document_id in self.document_chunks:
            await self.save_chunks(document_id)

    def _save_metadata_sync(self) -> None:
        """Sauvegarde synchrone des m√©tadonn√©es"""
        try:
            with open(self.metadata_path, 'w', encoding='utf-8') as f:
                json.dump(self.documents, f, indent=2)
        except Exception as e:
            print(f"Erreur lors de la sauvegarde des m√©tadonn√©es: {str(e)}")

    async def save_chunks(self, document_id: str) -> str:
        """
        Sauvegarde les chunks d'un document sur disque

        Args:
            document_id: ID du document

        Returns:
            Chemin du fichier de sauvegarde
        """
        if document_id not in self.document_chunks:
            return ""

        chunks = self.document_chunks[document_id]
        chunks_dir = os.path.join(self.storage_dir, "chunks")
        os.makedirs(chunks_dir, exist_ok=True)
        chunks_path = os.path.join(chunks_dir, f"{document_id}.pkl")

        # Sauvegarder de fa√ßon asynchrone
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            None,
            lambda: self._save_chunks_sync(chunks_path, chunks)
        )

        return chunks_path

    def _save_chunks_sync(self, path: str, chunks: List[Dict[str, Any]]) -> None:
        """Sauvegarde synchrone des chunks"""
        with open(path, 'wb') as f:
            pickle.dump(chunks, f)

    async def load_chunks(self, document_id: str) -> List[Dict[str, Any]]:
        """
        Charge les chunks d'un document depuis le disque

        Args:
            document_id: ID du document

        Returns:
            Liste des chunks ou liste vide si √©chec
        """
        chunks_path = os.path.join(self.storage_dir, "chunks", f"{document_id}.pkl")

        if not os.path.exists(chunks_path):
            print(f"Fichier de chunks non trouv√©: {chunks_path}")
            return []

        # Charger de fa√ßon asynchrone
        loop = asyncio.get_event_loop()
        chunks = await loop.run_in_executor(
            None,
            lambda: self._load_chunks_sync(chunks_path)
        )

        return chunks or []

    def _load_chunks_sync(self, path: str) -> Optional[List[Dict[str, Any]]]:
        """Chargement synchrone des chunks"""
        try:
            with open(path, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            print(f"Erreur lors du chargement des chunks: {str(e)}")
            return None

    async def add_document(self, filepath: str) -> str:
        """
        Ajoute un document au stockage

        Args:
            filepath: Chemin vers le document

        Returns:
            ID du document ajout√©
        """
        # Traiter le document
        document_id, chunks = await self.processor.process_document(filepath)

        # Copier le document dans le r√©pertoire de stockage
        filename = os.path.basename(filepath)
        document_path = os.path.join(self.documents_dir, f"{document_id}_{filename}")

        # Copier de fa√ßon asynchrone
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, lambda: shutil.copy2(filepath, document_path))

        # Stocker les m√©tadonn√©es
        self.documents[document_id] = {
            "id": document_id,
            "path": document_path,
            "original_path": filepath,
            "original_filename": filename,
            "chunks_count": len(chunks)
        }

        # Stocker les chunks
        self.document_chunks[document_id] = chunks

        # Cr√©er les embeddings
        await self.embedding_manager.create_embeddings(chunks)

        # Sauvegarder les embeddings
        await self.embedding_manager.save_embeddings(document_id)

        # Sauvegarder les m√©tadonn√©es
        await self._save_metadata()

        return document_id

    async def add_documents(self, filepaths: List[str]) -> List[str]:
        """
        Ajoute plusieurs documents au stockage

        Args:
            filepaths: Liste des chemins vers les documents

        Returns:
            Liste des IDs des documents ajout√©s
        """
        document_ids = []
        for filepath in filepaths:
            document_id = await self.add_document(filepath)
            document_ids.append(document_id)
        return document_ids

    async def remove_document(self, document_id: str) -> bool:
        """
        Supprime un document du stockage

        Args:
            document_id: ID du document √† supprimer

        Returns:
            True si la suppression a r√©ussi, False sinon
        """
        if document_id not in self.documents:
            return False

        # R√©cup√©rer le chemin du document
        document_path = self.documents[document_id]["path"]

        # Supprimer le fichier
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, lambda: os.remove(document_path) if os.path.exists(document_path) else None)

        # Supprimer les embeddings
        embedding_path = os.path.join(self.embedding_manager.storage_dir, f"{document_id}.pkl")
        await loop.run_in_executor(None, lambda: os.remove(embedding_path) if os.path.exists(embedding_path) else None)

        # Supprimer les m√©tadonn√©es et chunks
        del self.documents[document_id]
        if document_id in self.document_chunks:
            del self.document_chunks[document_id]

        # Sauvegarder les m√©tadonn√©es
        await self._save_metadata()

        return True

    async def get_document(self, document_id: str) -> Optional[Dict[str, Any]]:
        """R√©cup√®re les m√©tadonn√©es d'un document par son ID"""
        return self.documents.get(document_id)

    async def get_all_documents(self) -> Dict[str, Dict[str, Any]]:
        """R√©cup√®re tous les documents"""
        return self.documents

    async def get_document_chunks(self, document_id: str) -> Optional[List[Dict[str, Any]]]:
        """R√©cup√®re les chunks d'un document par son ID"""
        return self.document_chunks.get(document_id)

    async def old_load_document_chunks(self, document_id: str) -> bool:
        """
        Charge les chunks d'un document depuis son fichier

        Args:
            document_id: ID du document

        Returns:
            True si le chargement a r√©ussi, False sinon
        """
        if document_id not in self.documents:
            # print(f"Document {document_id} non trouv√© dans les m√©tadonn√©es.")
            return False

        # V√©rifier si les chunks sont d√©j√† charg√©s
        if document_id in self.document_chunks and self.document_chunks[document_id]:
            # print(f"Chunks du document {document_id} d√©j√† en m√©moire.")

            # V√©rifier quand m√™me si les embeddings sont charg√©s
            embedding_loaded = await self.embedding_manager.load_embeddings(document_id)

            return True

        # Essayer de charger les chunks depuis le fichier
        saved_chunks = await self.load_chunks(document_id)
        if saved_chunks:
            print(f"Chargement de {len(saved_chunks)} chunks sauvegard√©s pour le document {document_id}.")
            self.document_chunks[document_id] = saved_chunks

            # Charger les embeddings
            embedding_loaded = await self.embedding_manager.load_embeddings(document_id)
            if not embedding_loaded:
                print(f"Embeddings non trouv√©s. Cr√©ation des embeddings...")
                await self.embedding_manager.create_embeddings(saved_chunks)
                await self.embedding_manager.save_embeddings(document_id)

            return True

        # Si aucun chunk sauvegard√©, recr√©er les chunks

    async def load_document_chunks(self, document_id: str) -> bool:
        """
        Charge les chunks d'un document depuis son fichier avec v√©rification et r√©paration
        """
        if document_id not in self.documents:
            print(f"Document {document_id} non trouv√© dans metadata.json")
            return False

        # D√©j√† en m√©moire? V√©rifier les embeddings aussi
        if document_id in self.document_chunks and self.document_chunks[document_id]:
            #print(f"Chunks du document {document_id} d√©j√† en m√©moire.")

            # V√©rifier si les embeddings sont charg√©s
            embedding_path = os.path.join(self.embedding_manager.storage_dir, f"{document_id}.pkl")
            if os.path.exists(embedding_path):
                if not any(chunk_id.startswith(f"{document_id}-chunk-")
                           for chunk_id in self.embedding_manager.embeddings):
                    print(f"Chargement des embeddings pour {document_id}...")
                    await self.embedding_manager.load_embeddings(document_id)
            else:
                print(f"‚ö†Ô∏è Embeddings manquants pour {document_id}, cr√©ation...")
                await self.embedding_manager.create_embeddings(self.document_chunks[document_id])
                await self.embedding_manager.save_embeddings(document_id)

            return True

        # Chercher les chunks dans le fichier
        chunks_path = os.path.join(self.storage_dir, "chunks", f"{document_id}.pkl")
        if os.path.exists(chunks_path):
            saved_chunks = await self.load_chunks(document_id)
            if saved_chunks:
                print(f"Chargement de {len(saved_chunks)} chunks pour document {document_id}.")
                self.document_chunks[document_id] = saved_chunks

                # V√©rifier et charger/cr√©er les embeddings
                embedding_path = os.path.join(self.embedding_manager.storage_dir, f"{document_id}.pkl")
                if os.path.exists(embedding_path):
                    await self.embedding_manager.load_embeddings(document_id)
                else:
                    print(f"Embeddings non trouv√©s pour {document_id}, cr√©ation...")
                    await self.embedding_manager.create_embeddings(saved_chunks)
                    await self.embedding_manager.save_embeddings(document_id)

                return True

        # Recr√©er les chunks si n√©cessaire
        print(f"Chunks non trouv√©s pour {document_id}, recr√©ation...")

        document_path = self.documents[document_id]["path"]

        if not os.path.exists(document_path):
            print(f"Fichier du document {document_id} non trouv√©: {document_path}")
            return False

        print(f"Recr√©ation des chunks pour le document {document_id}...")
        try:
            _, chunks = await self.processor.process_document(document_path)

            # Stocker les chunks
            self.document_chunks[document_id] = chunks
            print(f"{len(chunks)} chunks cr√©√©s pour le document {document_id}.")

            # Sauvegarder les chunks
            await self.save_chunks(document_id)

            # Charger ou cr√©er les embeddings
            embedding_loaded = await self.embedding_manager.load_embeddings(document_id)
            if not embedding_loaded:
                print(f"Cr√©ation des embeddings pour le document {document_id}...")
                await self.embedding_manager.create_embeddings(chunks)
                await self.embedding_manager.save_embeddings(document_id)

            return True
        except Exception as e:
            print(f"Erreur lors du traitement du document {document_id}: {str(e)}")
            return False

    async def initialize(self) -> None:
        """Initialise le stockage en chargeant tous les documents et embeddings"""
        print(f"Initialisation du stockage de documents...")
        print(f"Documents trouv√©s dans les m√©tadonn√©es: {len(self.documents)}")

        # Charger les chunks et embeddings pour chaque document
        for document_id in self.documents:
            print(f"Chargement du document {document_id}...")
            success = await self.load_document_chunks(document_id)
            if success:
                print(f"Document {document_id} charg√© avec succ√®s.")
            else:
                print(f"Erreur lors du chargement du document {document_id}.")

